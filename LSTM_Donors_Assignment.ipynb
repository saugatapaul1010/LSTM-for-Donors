{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJaOZS-WaiyS"
   },
   "source": [
    "## 1. Importing the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37047,
     "status": "ok",
     "timestamp": 1565351658472,
     "user": {
      "displayName": "Saugata Paul",
      "photoUrl": "https://lh6.googleusercontent.com/-tUf1TiNn6hQ/AAAAAAAAAAI/AAAAAAAAA18/Qfv6wTX6nwA/s64/photo.jpg",
      "userId": "10977399337021875465"
     },
     "user_tz": -330
    },
    "id": "de6DdQHEaiyY",
    "outputId": "cf2c9523-5ff8-4009-8ff9-467bce0986d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "global start \n",
    "start = dt.now()\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BaJPh0ZYaiyn"
   },
   "source": [
    "## 2. Loading the data and displaying the initial tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hdEyOVRYaiyq"
   },
   "outputs": [],
   "source": [
    "#Load the given dataframes.\n",
    "project_data = pd.read_csv('train_data.csv')\n",
    "resource_data = pd.read_csv('resources.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hldnz1zaiyz",
    "outputId": "08459239-9a8e-438b-c7d9-294b2632065b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_submitted_datetime</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160221</td>\n",
       "      <td>p253737</td>\n",
       "      <td>c90749f5d961ff158d4b4d1e7dc665fc</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>IN</td>\n",
       "      <td>2016-12-05 13:43:57</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>ESL, Literacy</td>\n",
       "      <td>Educational Support for English Learners at Home</td>\n",
       "      <td>My students are English learners that are work...</td>\n",
       "      <td>\\\"The limits of your language are the limits o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need opportunities to practice beg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140945</td>\n",
       "      <td>p258326</td>\n",
       "      <td>897464ce9ddc600bced1151f324dd63a</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>FL</td>\n",
       "      <td>2016-10-25 09:22:10</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>History &amp; Civics, Health &amp; Sports</td>\n",
       "      <td>Civics &amp; Government, Team Sports</td>\n",
       "      <td>Wanted: Projector for Hungry Learners</td>\n",
       "      <td>Our students arrive to our school eager to lea...</td>\n",
       "      <td>The projector we need for our school is very c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need a projector to help with view...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                        teacher_id teacher_prefix  \\\n",
       "0      160221  p253737  c90749f5d961ff158d4b4d1e7dc665fc           Mrs.   \n",
       "1      140945  p258326  897464ce9ddc600bced1151f324dd63a            Mr.   \n",
       "\n",
       "  school_state project_submitted_datetime project_grade_category  \\\n",
       "0           IN        2016-12-05 13:43:57          Grades PreK-2   \n",
       "1           FL        2016-10-25 09:22:10             Grades 6-8   \n",
       "\n",
       "          project_subject_categories     project_subject_subcategories  \\\n",
       "0                Literacy & Language                     ESL, Literacy   \n",
       "1  History & Civics, Health & Sports  Civics & Government, Team Sports   \n",
       "\n",
       "                                      project_title  \\\n",
       "0  Educational Support for English Learners at Home   \n",
       "1             Wanted: Projector for Hungry Learners   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  My students are English learners that are work...   \n",
       "1  Our students arrive to our school eager to lea...   \n",
       "\n",
       "                                     project_essay_2 project_essay_3  \\\n",
       "0  \\\"The limits of your language are the limits o...             NaN   \n",
       "1  The projector we need for our school is very c...             NaN   \n",
       "\n",
       "  project_essay_4                           project_resource_summary  \\\n",
       "0             NaN  My students need opportunities to practice beg...   \n",
       "1             NaN  My students need a projector to help with view...   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \n",
       "0                                             0                    0  \n",
       "1                                             7                    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the contents of \"train_data.csv\"\n",
    "project_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2S9coMZAaiy7",
    "outputId": "5556dbac-239d-413b-b0ed-f5edbac3e65b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p233245</td>\n",
       "      <td>LC652 - Lakeshore Double-Space Mobile Drying Rack</td>\n",
       "      <td>1</td>\n",
       "      <td>149.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p069063</td>\n",
       "      <td>Bouncy Bands for Desks (Blue support pipes)</td>\n",
       "      <td>3</td>\n",
       "      <td>14.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                        description  quantity  \\\n",
       "0  p233245  LC652 - Lakeshore Double-Space Mobile Drying Rack         1   \n",
       "1  p069063        Bouncy Bands for Desks (Blue support pipes)         3   \n",
       "\n",
       "    price  \n",
       "0  149.00  \n",
       "1   14.95  "
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the contents of \"resources.csv\"\n",
    "resource_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zoGXnLw0aizE"
   },
   "source": [
    "## 3. Merge the two dataframes based on ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZM6eJLoaizF",
    "outputId": "89ee751e-9501-49fd-f3c5-a3d15e89918f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_submitted_datetime</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mrs.</td>\n",
       "      <td>IN</td>\n",
       "      <td>2016-12-05 13:43:57</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>ESL, Literacy</td>\n",
       "      <td>Educational Support for English Learners at Home</td>\n",
       "      <td>My students are English learners that are work...</td>\n",
       "      <td>\\\"The limits of your language are the limits o...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>My students need opportunities to practice beg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>154.60</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr.</td>\n",
       "      <td>FL</td>\n",
       "      <td>2016-10-25 09:22:10</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>History &amp; Civics, Health &amp; Sports</td>\n",
       "      <td>Civics &amp; Government, Team Sports</td>\n",
       "      <td>Wanted: Projector for Hungry Learners</td>\n",
       "      <td>Our students arrive to our school eager to lea...</td>\n",
       "      <td>The projector we need for our school is very c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>My students need a projector to help with view...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>299.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms.</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2016-08-31 12:03:56</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>Health &amp; Sports</td>\n",
       "      <td>Health &amp; Wellness, Team Sports</td>\n",
       "      <td>Soccer Equipment for AWESOME Middle School Stu...</td>\n",
       "      <td>\\r\\n\\\"True champions aren't always the ones th...</td>\n",
       "      <td>The students on the campus come to school know...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>My students need shine guards, athletic socks,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>516.85</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  teacher_prefix school_state project_submitted_datetime  \\\n",
       "0           Mrs.           IN        2016-12-05 13:43:57   \n",
       "1            Mr.           FL        2016-10-25 09:22:10   \n",
       "2            Ms.           AZ        2016-08-31 12:03:56   \n",
       "\n",
       "  project_grade_category         project_subject_categories  \\\n",
       "0          Grades PreK-2                Literacy & Language   \n",
       "1             Grades 6-8  History & Civics, Health & Sports   \n",
       "2             Grades 6-8                    Health & Sports   \n",
       "\n",
       "      project_subject_subcategories  \\\n",
       "0                     ESL, Literacy   \n",
       "1  Civics & Government, Team Sports   \n",
       "2    Health & Wellness, Team Sports   \n",
       "\n",
       "                                       project_title  \\\n",
       "0   Educational Support for English Learners at Home   \n",
       "1              Wanted: Projector for Hungry Learners   \n",
       "2  Soccer Equipment for AWESOME Middle School Stu...   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  My students are English learners that are work...   \n",
       "1  Our students arrive to our school eager to lea...   \n",
       "2  \\r\\n\\\"True champions aren't always the ones th...   \n",
       "\n",
       "                                     project_essay_2 project_essay_3  \\\n",
       "0  \\\"The limits of your language are the limits o...                   \n",
       "1  The projector we need for our school is very c...                   \n",
       "2  The students on the campus come to school know...                   \n",
       "\n",
       "  project_essay_4                           project_resource_summary  \\\n",
       "0                  My students need opportunities to practice beg...   \n",
       "1                  My students need a projector to help with view...   \n",
       "2                  My students need shine guards, athletic socks,...   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved   price  \\\n",
       "0                                             0                    0  154.60   \n",
       "1                                             7                    1  299.00   \n",
       "2                                             1                    0  516.85   \n",
       "\n",
       "   quantity  \n",
       "0        23  \n",
       "1         1  \n",
       "2        22  "
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merging both the dataframes by their corresponding IDs\n",
    "price_quantity_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\n",
    "project_data = pd.merge(project_data, price_quantity_data, on='id', how='left')\n",
    "\n",
    "#Remove the columns which are not needed anymore. Keeping ID for now.\n",
    "project_data.drop(['Unnamed: 0', 'teacher_id', 'id'], axis=1, inplace=True)\n",
    "\n",
    "project_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_oaj8OYVaizN"
   },
   "source": [
    "## 4. Basic information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vGnm3Y0vaizQ",
    "outputId": "824e068c-99de-40d6-8c28-e65ef9c38e81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points we have:  109248\n",
      "Number of initial features we have: 16\n",
      "\n",
      "Let's look at the all columns present in the dataset: \n",
      " ['teacher_prefix' 'school_state' 'project_submitted_datetime'\n",
      " 'project_grade_category' 'project_subject_categories'\n",
      " 'project_subject_subcategories' 'project_title' 'project_essay_1'\n",
      " 'project_essay_2' 'project_essay_3' 'project_essay_4'\n",
      " 'project_resource_summary' 'teacher_number_of_previously_posted_projects'\n",
      " 'project_is_approved' 'price' 'quantity']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data points we have: \", project_data.shape[0])\n",
    "print(\"Number of initial features we have:\", project_data.shape[1]) \n",
    "print(\"\\nLet's look at the all columns present in the dataset: \\n\",project_data.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79D-meRGaizZ"
   },
   "source": [
    "# 5. Data Pre-processing Section\n",
    "\n",
    "In this section, we will pre-process all the data before using them to build Machine Learning models. The dataset has the following types of features:\n",
    "\n",
    "<b>Categorical variables:</b>\n",
    "\n",
    "1. teacher_prefix\n",
    "2. school_state\n",
    "3. project_grade_category\n",
    "4. project_subject_categories\n",
    "5. project_subject_subcategories\n",
    "\n",
    "<b>Text data:</b>\n",
    "\n",
    "1. project_essay_1\n",
    "2. project_essay_2\n",
    "3. project_essay_3\n",
    "4. project_essay_4\n",
    "5. project_title\n",
    "6. project_resource_summary\n",
    "\n",
    "<b>Numerical Data:</b>\n",
    "\n",
    "1. teacher_number_of_previously_posted_projects\n",
    "2. price\n",
    "3. quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crhl71YJaize"
   },
   "source": [
    "### 5.1 Utility functions for pre processing text datas and categories data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEZQXGsMaizi"
   },
   "outputs": [],
   "source": [
    "#Stopwords contains the list of commonly found english keywords. We will remove them from the text datas as part of the pre-processing of data.\n",
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\", \\\n",
    "            't','u','v','w','x','y','z']\n",
    "\n",
    "from tqdm import tqdm\n",
    "def clean_subjects(input_values):\n",
    "    '''This function will be used to pre process the two features -> \"project_subject_categories\" and \n",
    "    \"project_subject_subcategories\"'''\n",
    "    processed_list = []\n",
    "    for i in tqdm(input_values):\n",
    "        temp = \"\"\n",
    "        for j in i.split(','):\n",
    "            if 'The' in j.split(): \n",
    "                j=j.replace('The','') \n",
    "            j = j.replace(' ','') \n",
    "            temp +=j.strip()+\" \"\n",
    "            temp = temp.replace('&','_')\n",
    "        processed_list.append(temp.strip())\n",
    "    return processed_list\n",
    "\n",
    "import re\n",
    "def decontracted(phrase):\n",
    "    \"\"\"This function will be used to expand the de-contracted words\"\"\"\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def process_text(list_of_sentences):\n",
    "    \"\"\"This function will be used to pre-process the text data\"\"\"\n",
    "    preprocessed_texts = []\n",
    "    for sentence in tqdm(list_of_sentences):\n",
    "        sent = decontracted(sentence)\n",
    "        sent = sent.replace('\\\\r', ' ')\n",
    "        sent = sent.replace('\\\\\"', ' ')\n",
    "        sent = sent.replace('\\\\n', ' ')\n",
    "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "        sent = ' '.join(word.lower() for word in sent.split() if word.lower() not in stopwords) #We will keep only those words in title which has a string length greater than one\n",
    "        preprocessed_texts.append(sent.lower().strip())\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nza7Oouaaizr"
   },
   "source": [
    "### 5.2 Pre-Processing the 'essays' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ZZoVxzkaizt",
    "outputId": "1e88fe81-81ea-45af-f4c8-66164b9daf4a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109248/109248 [02:00<00:00, 905.36it/s]\n"
     ]
    }
   ],
   "source": [
    "#Merging the essays data into one single column for ease of processing.\n",
    "project_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) + \\\n",
    "                        project_data[\"project_essay_2\"].map(str) + \\\n",
    "                        project_data[\"project_essay_3\"].map(str) + \\\n",
    "                        project_data[\"project_essay_4\"].map(str)\n",
    "\n",
    "preprocessed_essays = process_text(project_data[\"essay\"].values)\n",
    "\n",
    "#Add the pre-processed data into a new column.\n",
    "project_data['clean_essays'] = preprocessed_essays\n",
    "\n",
    "#Remove the columns which are already processed and are not needed anymore.\n",
    "project_data.drop(['essay','project_essay_1','project_essay_2','project_essay_3','project_essay_4'], \n",
    "                  axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ihEMwrGDaiz9"
   },
   "source": [
    "### 5.3 Pre-Processing the 'project_resource_summary' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlFP5Kg1ai0A",
    "outputId": "073b120b-f84d-43de-9538-95e11c8fa86d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109248/109248 [00:12<00:00, 8621.78it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_summary = process_text(project_data['project_resource_summary'].values)\n",
    "project_data['clean_project_resource_summary'] = preprocessed_summary\n",
    "project_data.drop(['project_resource_summary'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tPx_wGjai0K"
   },
   "source": [
    "### 5.4 Pre-Processing the 'project_title' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xBQSH868ai0L",
    "outputId": "590520fd-43a1-4337-fae6-0cfb26f7d52a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109248/109248 [00:05<00:00, 19912.64it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_titles = process_text(project_data['project_title'].values)\n",
    "project_data['clean_project_title'] = preprocessed_titles\n",
    "project_data.drop(['project_title'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fPxsuz1Bai0U"
   },
   "source": [
    "### 5.5 Pre-Processing of 'project_subject_categories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CVkOxbwLai0W",
    "outputId": "af0a69b4-6230-4621-a0e1-11e16dcf7075"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109248/109248 [00:00<00:00, 302094.48it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_categories = clean_subjects(project_data['project_subject_categories'].values)\n",
    "\n",
    "project_data['clean_categories'] = preprocessed_categories\n",
    "project_data.drop(['project_subject_categories'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gw__QsaLai0f"
   },
   "source": [
    "### 5.6 Pre-Processing of 'project_subject_subcategories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0XVBaTXai0h",
    "outputId": "8646133e-69c1-483f-8543-58be23bad130"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109248/109248 [00:00<00:00, 301790.06it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_subcategories = clean_subjects(project_data['project_subject_subcategories'].values)\n",
    "\n",
    "project_data['clean_subcategories'] = preprocessed_subcategories\n",
    "project_data.drop(['project_subject_subcategories'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "khlhBO5Sai0n"
   },
   "source": [
    "### 5.7 Pre-Processing of 'project_grade_category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WiAMO4uRai0p"
   },
   "outputs": [],
   "source": [
    "project_data['project_grade_category'] = project_data['project_grade_category'].map(lambda x: x.replace(\" \",\"_\").replace(\"-\",\"_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKBe8NZdai0v"
   },
   "source": [
    "### 5.7 Designing a new feature called - Presence of numerical digits in project resources summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jum04psJai0y"
   },
   "outputs": [],
   "source": [
    "def is_digit(sent):\n",
    "    digits=re.findall('\\d+', sent)\n",
    "    if(len(digits) != 0 ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "project_data['presence_of_the_numerical_digits']=project_data['clean_project_resource_summary'].apply(is_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t66oM48Yai06"
   },
   "source": [
    "### 5.8 Combining all text features into one single feature 'total_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOsWMQLxai1B"
   },
   "outputs": [],
   "source": [
    "project_data['total_text'] = project_data['clean_essays'].map(str) + \" \" + project_data['clean_project_resource_summary'].map(str) + \" \" + project_data['clean_project_title'].map(str)\n",
    "project_data.drop(['clean_essays','clean_project_resource_summary','clean_project_title','project_submitted_datetime'], axis=1, inplace=True) #Remove not needed features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eg9Y7ZZpai1H"
   },
   "source": [
    "### 5.9 Replacing NAN values by empty strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ckEz3yUai1J"
   },
   "outputs": [],
   "source": [
    "project_data['teacher_prefix'] = project_data['teacher_prefix'].fillna(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M4v0JMY9ai1T"
   },
   "source": [
    "### 5.10 Display the processed dataframe and save it into a pandas CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zItKXDt8ai1W",
    "outputId": "fa9051f9-04e8-4741-ce1b-18ea12344884"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>presence_of_the_numerical_digits</th>\n",
       "      <th>total_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mrs.</td>\n",
       "      <td>IN</td>\n",
       "      <td>Grades_PreK_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>154.60</td>\n",
       "      <td>23</td>\n",
       "      <td>Literacy_Language</td>\n",
       "      <td>ESL Literacy</td>\n",
       "      <td>0</td>\n",
       "      <td>students english learners working english seco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr.</td>\n",
       "      <td>FL</td>\n",
       "      <td>Grades_6_8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>299.00</td>\n",
       "      <td>1</td>\n",
       "      <td>History_Civics Health_Sports</td>\n",
       "      <td>Civics_Government TeamSports</td>\n",
       "      <td>0</td>\n",
       "      <td>students arrive school eager learn polite gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms.</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Grades_6_8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>516.85</td>\n",
       "      <td>22</td>\n",
       "      <td>Health_Sports</td>\n",
       "      <td>Health_Wellness TeamSports</td>\n",
       "      <td>0</td>\n",
       "      <td>true champions not always ones win guts mia ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mrs.</td>\n",
       "      <td>KY</td>\n",
       "      <td>Grades_PreK_2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>232.90</td>\n",
       "      <td>4</td>\n",
       "      <td>Literacy_Language Math_Science</td>\n",
       "      <td>Literacy Mathematics</td>\n",
       "      <td>0</td>\n",
       "      <td>work unique school filled esl english second l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mrs.</td>\n",
       "      <td>TX</td>\n",
       "      <td>Grades_PreK_2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67.98</td>\n",
       "      <td>4</td>\n",
       "      <td>Math_Science</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>0</td>\n",
       "      <td>second grade classroom next year made around 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  teacher_prefix school_state project_grade_category  \\\n",
       "0           Mrs.           IN          Grades_PreK_2   \n",
       "1            Mr.           FL             Grades_6_8   \n",
       "2            Ms.           AZ             Grades_6_8   \n",
       "3           Mrs.           KY          Grades_PreK_2   \n",
       "4           Mrs.           TX          Grades_PreK_2   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved   price  \\\n",
       "0                                             0                    0  154.60   \n",
       "1                                             7                    1  299.00   \n",
       "2                                             1                    0  516.85   \n",
       "3                                             4                    1  232.90   \n",
       "4                                             1                    1   67.98   \n",
       "\n",
       "   quantity                clean_categories           clean_subcategories  \\\n",
       "0        23               Literacy_Language                  ESL Literacy   \n",
       "1         1    History_Civics Health_Sports  Civics_Government TeamSports   \n",
       "2        22                   Health_Sports    Health_Wellness TeamSports   \n",
       "3         4  Literacy_Language Math_Science          Literacy Mathematics   \n",
       "4         4                    Math_Science                   Mathematics   \n",
       "\n",
       "   presence_of_the_numerical_digits  \\\n",
       "0                                 0   \n",
       "1                                 0   \n",
       "2                                 0   \n",
       "3                                 0   \n",
       "4                                 0   \n",
       "\n",
       "                                          total_text  \n",
       "0  students english learners working english seco...  \n",
       "1  students arrive school eager learn polite gene...  \n",
       "2  true champions not always ones win guts mia ha...  \n",
       "3  work unique school filled esl english second l...  \n",
       "4  second grade classroom next year made around 2...  "
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After pre-processing, we have 13 features. We will have one feature as our target variable and the rest of the columns as our independent features variables.\n",
    "project_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gHFOrfNJOzCe"
   },
   "outputs": [],
   "source": [
    "DATADIR = \"/content/drive/My Drive/Donors/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euR2NTDQai1e"
   },
   "outputs": [],
   "source": [
    "#Save the processed dataset into a pandas CSV file.\n",
    "project_data.to_csv(\"processed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SW1SmPRmai1p"
   },
   "outputs": [],
   "source": [
    "project_data = pd.read_csv(\"processed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yXT8isuoai11"
   },
   "source": [
    "## 6. Splitting the original data into train and test data in 80:20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2026,
     "status": "ok",
     "timestamp": 1565351900815,
     "user": {
      "displayName": "Saugata Paul",
      "photoUrl": "https://lh6.googleusercontent.com/-tUf1TiNn6hQ/AAAAAAAAAAI/AAAAAAAAA18/Qfv6wTX6nwA/s64/photo.jpg",
      "userId": "10977399337021875465"
     },
     "user_tz": -330
    },
    "id": "lpFYBxJlai12",
    "outputId": "056384c7-a324-40ff-8d68-91e27dfaadc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in train data:  87398\n",
      "Number of points in validation data:  17480\n",
      "Number of points in test data:  4370\n"
     ]
    }
   ],
   "source": [
    "#Taking the target and predictor variables into separate variables\n",
    "y = project_data[\"project_is_approved\"] #target variables\n",
    "X = project_data.drop(['project_is_approved'], axis=1) #predictor variables\n",
    "\n",
    "#Split the dataset into train and val dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.20, random_state=1, stratify=y_test)\n",
    "\n",
    "#Display basic information after splitting the data\n",
    "print(\"Number of points in train data: \",X_train.shape[0])\n",
    "print(\"Number of points in validation data: \",X_val.shape[0])\n",
    "print(\"Number of points in test data: \",X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Zw_CISrai1-"
   },
   "source": [
    "# Model-1\n",
    "\n",
    "Build and Train deep neural network as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2Y3rX-7ai1_"
   },
   "source": [
    "<img src='LSTM_Image_2.png'>\n",
    "ref: https://i.imgur.com/w395Yk9.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BfY-OM0ai2A"
   },
   "source": [
    "- __Input_seq_total_text_data__ --- You have to give Total text data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. \n",
    "- __Input_school_state__ --- Give 'school_state' column as input to embedding layer and Train the Keras Embedding layer. \n",
    "- __Project_grade_category__  --- Give 'project_grade_category' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_categories__ --- Give 'input_clean_categories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_clean_subcategories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_teacher_prefix' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_remaining_teacher_number_of_previously_posted_projects._resource_summary_contains_numerical_digits._price._quantity__ ---concatenate remaining columns and add a Dense layer after that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnUcU3gNai2E"
   },
   "source": [
    "- For LSTM, you can choose your sequence padding methods on your own or you can train your LSTM without padding, there is no restriction on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7nxtk2Aai2J"
   },
   "source": [
    "Below is an example of embedding layer for a categorical columns. In below code all are dummy values, we gave only for referance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3nWVHfPai2K"
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(n,))\n",
    "embedding = Embedding(no_1, no_2, input_length=n)(input_layer)\n",
    "flatten = Flatten()(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJMWdFC1ai2W"
   },
   "source": [
    "### 1. Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "### 2. Please go through this link https://keras.io/getting-started/functional-api-guide/ and check the 'Multi-input and multi-output models' then you will get to know how to give multiple inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3efbZmgtai2Z"
   },
   "source": [
    "## Total Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FFmK5N1ai2a"
   },
   "source": [
    "### Build train and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJuU959Rai2b"
   },
   "outputs": [],
   "source": [
    "#Get the total_text values in list\n",
    "docs_text_train=list(X_train.total_text.values)\n",
    "docs_text_val=list(X_val.total_text.values)\n",
    "docs_text_test=list(X_test.total_text.values)\n",
    "labels_train=np.array(y_train)\n",
    "labels_val=np.array(y_val)\n",
    "labels_test=np.array(y_test)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_text_train)\n",
    "\n",
    "#Convert the texts to sequences using the tokenizer\n",
    "sequences_text_train = tokens.texts_to_sequences(docs_text_train)\n",
    "sequences_text_val = tokens.texts_to_sequences(docs_text_val)\n",
    "sequences_text_test = tokens.texts_to_sequences(docs_text_test)\n",
    "vocab_size_text = len(tokens.word_index) + 1\n",
    "\n",
    "#Add padding\n",
    "padded_text_train = pad_sequences(sequences_text_train, maxlen=300, padding='post')\n",
    "padded_text_val = pad_sequences(sequences_text_val, maxlen=300, padding='post')\n",
    "padded_text_test = pad_sequences(sequences_text_test, maxlen=300, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9o0coef4ai2e",
    "outputId": "7f2792af-3aa8-43a3-9e26-4c10aa15939f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55638/55638 [00:00<00:00, 283171.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "file = open('glove.6B.300d.txt')\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "file.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#Create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size_text, 300))\n",
    "for word, i in tqdm(tokens.word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector \n",
    "\n",
    "print(len(embedding_matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2ZFIx0Pai2i",
    "outputId": "0ba24335-248f-48d3-e58d-ef1e2e19f70d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Get the flattened LSTM output for input text\n",
    "input_layer_total_text = Input(shape=(300,), name = \"total_text_sequence\")\n",
    "embedding_layer_total_text = Embedding(input_dim=vocab_size_text, output_dim=300, weights=[embedding_matrix], trainable=False)(input_layer_total_text)\n",
    "lstm_total_text  = LSTM(16, activation=\"relu\", return_sequences=True)(embedding_layer_total_text)\n",
    "flatten_total_text = Flatten()(lstm_total_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJTGS6FBai2m"
   },
   "source": [
    "## Categorical data: school_state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHTst5Fjai2o"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-e-0r67-ai2o"
   },
   "outputs": [],
   "source": [
    "#Get the school state values\n",
    "docs_school_state_train=list(X_train.school_state.values)\n",
    "docs_school_state_val=list(X_val.school_state.values)\n",
    "docs_school_state_test=list(X_test.school_state.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_school_state_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_school_train = np.array(tokens.texts_to_sequences(docs_school_state_train))\n",
    "sequences_school_val = np.array(tokens.texts_to_sequences(docs_school_state_val))\n",
    "sequences_school_test = np.array(tokens.texts_to_sequences(docs_school_state_test))\n",
    "vocab_size_school_state = len(tokens.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7eBcyZOai2q"
   },
   "outputs": [],
   "source": [
    "#Get the flattened output for school_state\n",
    "input_layer_school_state = Input(shape=(1,), name = \"encoded_school_state\")\n",
    "embedding_layer_school_state = Embedding(input_dim=vocab_size_school_state, output_dim=4, trainable=True)(input_layer_school_state)\n",
    "flatten_school_state = Flatten()(embedding_layer_school_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JeTeDK3Fai2w"
   },
   "source": [
    "## Categorical data: teacher_prefix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMiVgiHXai2x"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xtyjWIZWai2y"
   },
   "outputs": [],
   "source": [
    "#Get the teacher_prefix values\n",
    "docs_teacher_prefix_train=list(X_train.teacher_prefix.values)\n",
    "docs_teacher_prefix_val=list(X_val.teacher_prefix.values)\n",
    "docs_teacher_prefix_test=list(X_test.teacher_prefix.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_teacher_prefix_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_teacher_prefix_train = np.array(tokens.texts_to_sequences(docs_teacher_prefix_train))\n",
    "sequences_teacher_prefix_val = np.array(tokens.texts_to_sequences(docs_teacher_prefix_val))\n",
    "sequences_teacher_prefix_test = np.array(tokens.texts_to_sequences(docs_teacher_prefix_test))\n",
    "vocab_size_teacher_prefix = len(tokens.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m5yudtSNai21"
   },
   "outputs": [],
   "source": [
    "#Get the flattened output for teacher_prefix\n",
    "input_layer_teacher_prefix = Input(shape=(1,), name = \"teacher_prefix\")\n",
    "embedding_layer_teacher_prefix = Embedding(input_dim=vocab_size_teacher_prefix, output_dim=4, trainable=True)(input_layer_teacher_prefix)\n",
    "flatten_teacher_prefix = Flatten()(embedding_layer_teacher_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCkgeOPgai28"
   },
   "source": [
    "## Categorical data: project_grade_category "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PpyPFB7dai2-"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PB7nbUYMai2-"
   },
   "outputs": [],
   "source": [
    "#Get the project_grade_category values\n",
    "docs_project_grade_category_train=list(X_train.project_grade_category.values)\n",
    "docs_project_grade_category_val=list(X_val.project_grade_category.values)\n",
    "docs_project_grade_category_test=list(X_test.project_grade_category.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_project_grade_category_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_project_grade_category_train = tokens.texts_to_sequences(docs_project_grade_category_train)\n",
    "sequences_project_grade_category_val = tokens.texts_to_sequences(docs_project_grade_category_val)\n",
    "sequences_project_grade_category_test = tokens.texts_to_sequences(docs_project_grade_category_test)\n",
    "vocab_size_project_grade_category= len(tokens.word_index) + 1\n",
    "\n",
    "#Add padding\n",
    "padded_project_grade_category_train = pad_sequences(sequences_project_grade_category_train, maxlen=3, padding='post')\n",
    "padded_project_grade_category_val = pad_sequences(sequences_project_grade_category_val, maxlen=3, padding='post')\n",
    "padded_project_grade_category_test = pad_sequences(sequences_project_grade_category_test, maxlen=3, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOP-jcuLai3D"
   },
   "outputs": [],
   "source": [
    "#Get the flattened output for project_grade_category\n",
    "input_layer_project_grade = Input(shape=(3,), name = \"project_grade_category\")\n",
    "embedding_layer_project_grade = Embedding(input_dim=vocab_size_project_grade_category, output_dim=4, trainable=True)(input_layer_project_grade)\n",
    "flatten_project_grade = Flatten()(embedding_layer_project_grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6zMvdRNEai3F"
   },
   "source": [
    "## Categorical data: clean_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lah8psutai3M"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HCy7QHwai3N"
   },
   "outputs": [],
   "source": [
    "#Get the clean_categories values\n",
    "docs_clean_categories_train=list(X_train.clean_categories.values)\n",
    "docs_clean_categories_val=list(X_val.clean_categories.values)\n",
    "docs_clean_categories_test=list(X_test.clean_categories.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_clean_categories_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_clean_categories_train = tokens.texts_to_sequences(docs_clean_categories_train)\n",
    "sequences_clean_categories_val = tokens.texts_to_sequences(docs_clean_categories_val)\n",
    "sequences_clean_categories_test = tokens.texts_to_sequences(docs_clean_categories_test)\n",
    "vocab_size_clean_categories = len(tokens.word_index) + 1\n",
    "\n",
    "#Add padding\n",
    "padded_clean_categories_train = pad_sequences(sequences_clean_categories_train, maxlen=3, padding='post')\n",
    "padded_clean_categories_val = pad_sequences(sequences_clean_categories_val, maxlen=3, padding='post')\n",
    "padded_clean_categories_test = pad_sequences(sequences_clean_categories_test, maxlen=3, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHP5Z3KLai3R"
   },
   "outputs": [],
   "source": [
    "#Get the flattened output for clean_categories\n",
    "input_layer_clean_categories = Input(shape=(3,), name = \"clean_categories\")\n",
    "embedding_layer_clean_categories = Embedding(input_dim=vocab_size_clean_categories, output_dim=4, trainable=True)(input_layer_clean_categories)\n",
    "flatten_clean_categories = Flatten()(embedding_layer_clean_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1uyhOWnXai3V"
   },
   "source": [
    "## Categorical data: clean_subcategories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qzBXMoWhai3W"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbyDeZi8ai3Y"
   },
   "outputs": [],
   "source": [
    "#Get the clean_subcategories values\n",
    "docs_clean_subcategories_train=list(X_train.clean_subcategories.values)\n",
    "docs_clean_subcategories_val=list(X_val.clean_subcategories.values)\n",
    "docs_clean_subcategories_test=list(X_test.clean_subcategories.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_clean_subcategories_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_clean_subcategories_train = tokens.texts_to_sequences(docs_clean_subcategories_train)\n",
    "sequences_clean_subcategories_val = tokens.texts_to_sequences(docs_clean_subcategories_val)\n",
    "sequences_clean_subcategories_test = tokens.texts_to_sequences(docs_clean_subcategories_test)\n",
    "vocab_size_clean_subcategories = len(tokens.word_index) + 1\n",
    "\n",
    "padded_clean_subcategories_train = pad_sequences(sequences_clean_subcategories_train, maxlen=3, padding='post')\n",
    "padded_clean_subcategories_val = pad_sequences(sequences_clean_subcategories_val, maxlen=3, padding='post')\n",
    "padded_clean_subcategories_test = pad_sequences(sequences_clean_subcategories_test, maxlen=3, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PGjvMJd6ai3f"
   },
   "outputs": [],
   "source": [
    "#Get the flattened output for clean_subcategories\n",
    "input_layer_clean_subcategories = Input(shape=(3,), name = \"clean_subcategories\")\n",
    "embedding_layer_clean_subcategories = Embedding(input_dim=vocab_size_clean_subcategories, output_dim=4, trainable=True)(input_layer_clean_subcategories)\n",
    "flatten_clean_subcategories = Flatten()(embedding_layer_clean_subcategories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4fXpueg9ai3n"
   },
   "source": [
    "## Numerical datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9rO0dgCai3p"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def normalize_vars(data):\n",
    "    \"\"\"This function is used to normalize all the input datas between 0 and 1\"\"\"\n",
    "    normalizer = Normalizer()\n",
    "    data_normalized = normalizer.fit_transform(data.reshape(1, -1))\n",
    "    return data_normalized, normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAk4eNjzai3s"
   },
   "source": [
    "## teacher_number_of_previously_posted_projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89QmnErNai3t"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLbAF8I1ai3t"
   },
   "outputs": [],
   "source": [
    "previous_projects_train = X_train.teacher_number_of_previously_posted_projects.values\n",
    "previous_projects_val = X_val.teacher_number_of_previously_posted_projects.values\n",
    "previous_projects_test = X_test.teacher_number_of_previously_posted_projects.values\n",
    "\n",
    "norm_previous_projects_train, normalizer = normalize_vars(previous_projects_train.reshape(1,-1))\n",
    "norm_previous_projects_val = normalizer.transform(previous_projects_val.reshape(1,-1))\n",
    "norm_previous_projects_test = normalizer.transform(previous_projects_test.reshape(1,-1))\n",
    "\n",
    "norm_previous_projects_train = norm_previous_projects_train.reshape(len(X_train),1)\n",
    "norm_previous_projects_val = norm_previous_projects_val.reshape(len(X_val),1)\n",
    "norm_previous_projects_test = norm_previous_projects_test.reshape(len(X_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TeVCF_XSai30"
   },
   "outputs": [],
   "source": [
    "#Input layer for teacher_number_of_previously_posted_projects\n",
    "input_layer_previous_projects = Input(shape=(1,), name = \"previous_projects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yvgXs6_Fai33"
   },
   "source": [
    "## price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zPUjf6JJai34"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pDdzZIuai34"
   },
   "outputs": [],
   "source": [
    "price_train = X_train.price.values\n",
    "price_val = X_val.price.values\n",
    "price_test = X_test.price.values\n",
    "\n",
    "norm_price_train, normalizer = normalize_vars(price_train.reshape(1,-1))\n",
    "norm_price_val = normalizer.transform(price_val.reshape(1,-1))\n",
    "norm_price_test = normalizer.transform(price_test.reshape(1,-1))\n",
    "\n",
    "norm_price_train = norm_price_train.reshape(len(X_train),1)\n",
    "norm_price_val = norm_price_val.reshape(len(X_val),1)\n",
    "norm_price_test = norm_price_test.reshape(len(X_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRid_bpPai36"
   },
   "outputs": [],
   "source": [
    "#Input layer for price\n",
    "input_layer_price = Input(shape=(1,), name = \"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2xwf3Onai39"
   },
   "source": [
    "## quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M9d5Jmv-ai3_"
   },
   "source": [
    "### Building train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__WrelVhai3_"
   },
   "outputs": [],
   "source": [
    "quantity_train = X_train.quantity.values\n",
    "quantity_val = X_val.quantity.values\n",
    "quantity_test = X_test.quantity.values\n",
    "\n",
    "norm_quantity_train, normalizer = normalize_vars(quantity_train.reshape(1,-1))\n",
    "norm_quantity_val = normalizer.transform(quantity_val.reshape(1,-1))\n",
    "norm_quantity_test = normalizer.transform(quantity_test.reshape(1,-1))\n",
    "\n",
    "norm_quantity_train = norm_quantity_train.reshape(len(X_train),1)\n",
    "norm_quantity_val = norm_quantity_val.reshape(len(X_val),1)\n",
    "norm_quantity_test = norm_quantity_test.reshape(len(X_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uzh7T0TCai4C"
   },
   "outputs": [],
   "source": [
    "#Input layer for quantity\n",
    "input_layer_quantity = Input(shape=(1,), name = \"quantity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GoOmJA6gai4E"
   },
   "source": [
    "### Concatenation of all the numerical features layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhXbFch_ai4F"
   },
   "outputs": [],
   "source": [
    "numerical_features_layers_concat = concatenate([input_layer_previous_projects, input_layer_price, input_layer_quantity])\n",
    "dense_layer_numerical = Dense(4, activation='relu',kernel_initializer='he_normal')(numerical_features_layers_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FDtbyIQfai4H"
   },
   "source": [
    "### Concatenation of all the layers and building the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hU3E9qtzai4I",
    "outputId": "75bc4dd0-7735-4b1d-98f7-4bfa6e5fd142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "total_text_sequence (InputLayer (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 300, 300)     16691700    total_text_sequence[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "teacher_prefix (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoded_school_state (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "project_grade_category (InputLa (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_categories (InputLayer)   (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_subcategories (InputLayer (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "previous_projects (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "price (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "quantity (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300, 16)      20288       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 4)         24          teacher_prefix[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 4)         208         encoded_school_state[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 3, 4)         40          project_grade_category[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 3, 4)         64          clean_categories[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 3, 4)         152         clean_subcategories[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3)            0           previous_projects[0][0]          \n",
      "                                                                 price[0][0]                      \n",
      "                                                                 quantity[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4800)         0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 4)            0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 4)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12)           0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 12)           0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 12)           0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            16          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 4848)         0           flatten_1[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_1 (Dense)           (None, 8)            38792       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8)            0           dense_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_2 (Dense)           (None, 4)            36          dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4)            0           dense_layer_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 1)            5           dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 16,751,325\n",
      "Trainable params: 59,625\n",
      "Non-trainable params: 16,691,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Merge all the layers according to the architecture diagram\n",
    "x = concatenate([flatten_total_text, flatten_teacher_prefix, flatten_school_state, flatten_project_grade, flatten_clean_categories, flatten_clean_subcategories, dense_layer_numerical])\n",
    "x = Dense(8, activation='relu',kernel_initializer='he_normal', name='dense_layer_1')(x)\n",
    "x = Dropout(0.3, name='dropout_1')(x)\n",
    "x = Dense(4, activation='relu',kernel_initializer='he_normal',name='dense_layer_2')(x)\n",
    "x = Dropout(0.3, name='dropout_2')(x)\n",
    "output_layer = Dense(1, activation='sigmoid', name='output_layer')(x)\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=[input_layer_total_text,input_layer_teacher_prefix,input_layer_school_state,input_layer_project_grade,input_layer_clean_categories,\n",
    "                      input_layer_clean_subcategories,input_layer_previous_projects,input_layer_price,input_layer_quantity], outputs=[output_layer])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmrmlFZlai4R"
   },
   "source": [
    "### Defining a custom metric AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lep66tvJai4R"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#https://stackoverflow.com/questions/51922500/tf-metrics-auc-yielding-very-different-from-sklearn-metrics-roc-auc-score\n",
    "def roc_auc(y_true, y_pred):\n",
    "    auc = tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "    #auc = tf.metrics.auc(y_true, y_pred, num_thresholds=200)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "78xqxX9Tai4T"
   },
   "source": [
    "### Defining a callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rp1PleN6ai4T"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, EarlyStopping\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time))\n",
    "#early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto', baseline=None, restore_best_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SgFurg8rai4W"
   },
   "source": [
    "### Compiling the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xG2D--jDai4Y",
    "outputId": "b3af2850-4bc1-4e99-adca-5aa4ac605b89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-3a8aa8280c4d>:6: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 87398 samples, validate on 17480 samples\n",
      "Epoch 1/12\n",
      "87398/87398 [==============================] - 130s 1ms/step - loss: 0.4693 - roc_auc: 0.5692 - val_loss: 0.4044 - val_roc_auc: 0.7076\n",
      "Epoch 2/12\n",
      "87398/87398 [==============================] - 130s 1ms/step - loss: 0.4371 - roc_auc: 0.6377 - val_loss: 0.4027 - val_roc_auc: 0.7211\n",
      "Epoch 3/12\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.4274 - roc_auc: 0.6525 - val_loss: 0.3973 - val_roc_auc: 0.7302\n",
      "Epoch 4/12\n",
      "87398/87398 [==============================] - 134s 2ms/step - loss: 0.4187 - roc_auc: 0.6653 - val_loss: 0.3959 - val_roc_auc: 0.7315\n",
      "Epoch 5/12\n",
      "87398/87398 [==============================] - 131s 2ms/step - loss: 0.4114 - roc_auc: 0.6772 - val_loss: 0.3823 - val_roc_auc: 0.7353\n",
      "Epoch 6/12\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.4073 - roc_auc: 0.6817 - val_loss: 0.3824 - val_roc_auc: 0.7345\n",
      "Epoch 7/12\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.4017 - roc_auc: 0.6930 - val_loss: 0.3829 - val_roc_auc: 0.7342\n",
      "Epoch 8/12\n",
      "87398/87398 [==============================] - 125s 1ms/step - loss: 0.3989 - roc_auc: 0.6960 - val_loss: 0.4064 - val_roc_auc: 0.7222\n",
      "Epoch 9/12\n",
      "87398/87398 [==============================] - 125s 1ms/step - loss: 0.3937 - roc_auc: 0.7058 - val_loss: 0.3853 - val_roc_auc: 0.7299\n",
      "Epoch 10/12\n",
      "87398/87398 [==============================] - 125s 1ms/step - loss: 0.3902 - roc_auc: 0.7157 - val_loss: 0.3889 - val_roc_auc: 0.7258\n",
      "Epoch 11/12\n",
      "87398/87398 [==============================] - 128s 1ms/step - loss: 0.3865 - roc_auc: 0.7268 - val_loss: 0.3856 - val_roc_auc: 0.7271\n",
      "Epoch 12/12\n",
      "87398/87398 [==============================] - 133s 2ms/step - loss: 0.3855 - roc_auc: 0.7303 - val_loss: 0.3965 - val_roc_auc: 0.7232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efd04d996d8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[roc_auc])\n",
    "model.fit(x=[padded_text_train,sequences_teacher_prefix_train,sequences_school_train,padded_project_grade_category_train,padded_clean_categories_train,padded_clean_subcategories_train,\n",
    "             norm_previous_projects_train,norm_price_train,norm_quantity_train], \n",
    "          y=[labels_train],\n",
    "          validation_data=([padded_text_val,sequences_teacher_prefix_val,sequences_school_val,padded_project_grade_category_val,padded_clean_categories_val,padded_clean_subcategories_val,\n",
    "                            norm_previous_projects_val,norm_price_val,norm_quantity_val],[labels_val]),\n",
    "          epochs=12, \n",
    "          batch_size=1024, \n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNHC1rMmai4b",
    "outputId": "9fa236ef-48e9-44ec-ab27-ebbc92e87942"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.13.1 at http://saugata:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjjT1XgJai4d"
   },
   "source": [
    "<img src='Model1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on unseen test data:  0.7164852592744682\n",
      "Accuracy on unseen test data:  0.848512585812357\n"
     ]
    }
   ],
   "source": [
    "test_data=[padded_text_test,sequences_teacher_prefix_test,sequences_school_test,padded_project_grade_category_test,\n",
    "                  padded_clean_categories_test,padded_clean_subcategories_test,norm_previous_projects_test,norm_price_test,norm_quantity_test]\n",
    "\n",
    "#Test AUC\n",
    "y_pred= model.predict(test_data)\n",
    "print(\"AUC on unseen test data: \",roc_auc_score(y_test,y_pred))\n",
    "\n",
    "#Test Accuracy\n",
    "pred = [y_pred[i][0] for i in range(y_pred.shape[0])]\n",
    "class_pred = [1 if i>0.5 else 0 for i in pred]\n",
    "print(\"Accuracy on unseen test data: \",accuracy_score(y_test,class_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "model.save(\"model1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnaFkvfuai4d"
   },
   "source": [
    "# Model-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y9E0fdeoai4e"
   },
   "source": [
    "1. Train the TF-IDF on the Train data <br>\n",
    "2. Get the idf value for each word we have in the train data. <br>\n",
    "3. Remove the low idf value and high idf value words from our data. Do some analysis on the Idf values and based on those values choose the low and high threshold value. Because very frequent words and very very rare words don't give much information. (you can plot a box plots and take only the idf scores within IQR range and corresponding words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LmVWmQdVai4f"
   },
   "source": [
    "### TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_7x0wkjai4i",
    "outputId": "f025cf6a-e364-48a5-9f21-e9d907d6a6ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words present originally: 55628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Fit the TFIDF vectorizer to the train data\n",
    "vect = TfidfVectorizer()\n",
    "vect.fit_transform(X_train['total_text'])\n",
    "\n",
    "#Get the features names and their corresponding IDF scores\n",
    "words = vect.get_feature_names()\n",
    "idf_words = vect.idf_\n",
    "\n",
    "#Map the words and their idf_ scores in a disctionary\n",
    "dict_word_idf_ = dict(zip(words, idf_words))\n",
    "\n",
    "print(\"Total number of unique words present originally:\", len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R7LdTxPJai4o"
   },
   "source": [
    "### Box-Plot Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rF3uj-X5ai4p",
    "outputId": "7293a0e1-ad67-4f72-ec10-eeaa74c64e6b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest significant value of TF-IDF Scores:  6.5\n",
      "The highest significant value of TF-IDF Scores:  11.685091939370627\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.boxplot(idf_words)\n",
    "plt.title('Box Plot Analysis')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('IDF Score')\n",
    "plt.show()\n",
    "\n",
    "p_25th = 6.5\n",
    "p_75th = np.percentile(idf_words,75)\n",
    "\n",
    "print(\"The lowest significant value of TF-IDF Scores: \",p_25th)\n",
    "print(\"The highest significant value of TF-IDF Scores: \",p_75th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHGmfZBaai4u"
   },
   "source": [
    "### Create a list of words to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8222dhHai4w",
    "outputId": "bc21c165-1e87-4a26-de8e-033b420a134a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words to be removed:  2907\n"
     ]
    }
   ],
   "source": [
    "removed_wordlist = []\n",
    "for word in list(dict_word_idf_.keys()):\n",
    "    if(dict_word_idf_[word] < p_25th or dict_word_idf_[word] > p_75th):\n",
    "        removed_wordlist.append(word)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print(\"Number of words to be removed: \",len(removed_wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFfrRpDUai43"
   },
   "source": [
    "### Removing words from the train and val data which falls outside the threshold range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xtHTJoKrai43",
    "outputId": "0d53a575-95ce-4233-cf10-d1772bbb787d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87398/87398 [08:39<00:00, 168.12it/s]\n",
      "100%|██████████| 21850/21850 [02:11<00:00, 167.46it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_from_text(list_of_sentences):\n",
    "    \"\"\"This function will be used to remove words from text data\"\"\"\n",
    "    processed_text = []\n",
    "    for sentence in tqdm(list_of_sentences):\n",
    "        sent = ' '.join(word for word in sentence.split() if word not in removed_wordlist) #We will keep only those words in title which has a string length greater than one\n",
    "        processed_text.append(sent)\n",
    "    return processed_text\n",
    "\n",
    "X_train['total_text'] = remove_from_text(X_train.total_text.values)\n",
    "X_val['total_text'] = remove_from_text(X_val.total_text.values)\n",
    "X_test['total_text'] = remove_from_text(X_test.total_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1hmyUUjai49"
   },
   "outputs": [],
   "source": [
    "X_train.to_csv(\"X_train_removed.csv\", index=False)\n",
    "X_val.to_csv(\"X_val_removed.csv\", index=False)\n",
    "X_test.to_csv(\"X_test_removed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F8FjCcBzai5D"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"X_train_removed.csv\")\n",
    "X_val = pd.read_csv(\"X_val_removed.csv\")\n",
    "X_test = pd.read_csv(\"X_test_removed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3efbZmgtai2Z"
   },
   "source": [
    "## Total Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FFmK5N1ai2a"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJuU959Rai2b"
   },
   "outputs": [],
   "source": [
    "#Get the total_text values in list\n",
    "docs_text_train=list(X_train.total_text.values)\n",
    "docs_text_val=list(X_val.total_text.values)\n",
    "docs_text_test=list(X_test.total_text.values)\n",
    "labels_train=np.array(y_train)\n",
    "labels_val=np.array(y_val)\n",
    "labels_test=np.array(y_test)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_text_train)\n",
    "\n",
    "#Convert the texts to sequences using the tokenizer\n",
    "sequences_text_train = tokens.texts_to_sequences(docs_text_train)\n",
    "sequences_text_val = tokens.texts_to_sequences(docs_text_val)\n",
    "sequences_text_test = tokens.texts_to_sequences(docs_text_test)\n",
    "vocab_size_text = len(tokens.word_index) + 1\n",
    "\n",
    "#Add padding\n",
    "padded_text_train = pad_sequences(sequences_text_train, maxlen=300, padding='post')\n",
    "padded_text_val = pad_sequences(sequences_text_val, maxlen=300, padding='post')\n",
    "padded_text_test = pad_sequences(sequences_text_test, maxlen=300, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9o0coef4ai2e",
    "outputId": "7f2792af-3aa8-43a3-9e26-4c10aa15939f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 23363/55638 [00:00<00:00, 233627.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55638/55638 [00:00<00:00, 271418.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "file = open('glove.6B.300d.txt')\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "file.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#Create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size_text, 300))\n",
    "for word, i in tqdm(tokens.word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector \n",
    "\n",
    "print(len(embedding_matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2ZFIx0Pai2i",
    "outputId": "0ba24335-248f-48d3-e58d-ef1e2e19f70d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Get the flattened LSTM output for input text\n",
    "input_layer_total_text = Input(shape=(300,), name = \"total_text_sequence\")\n",
    "embedding_layer_total_text = Embedding(input_dim=vocab_size_text, output_dim=300, weights=[embedding_matrix], trainable=False)(input_layer_total_text)\n",
    "lstm_total_text  = LSTM(16, activation=\"relu\", return_sequences=True)(embedding_layer_total_text)\n",
    "flatten_total_text = Flatten()(lstm_total_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJTGS6FBai2m"
   },
   "source": [
    "## Categorical data: school_state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHTst5Fjai2o"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-e-0r67-ai2o"
   },
   "outputs": [],
   "source": [
    "#Get the school state values\n",
    "docs_school_state_train=list(X_train.school_state.values)\n",
    "docs_school_state_val=list(X_val.school_state.values)\n",
    "docs_school_state_test=list(X_test.school_state.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_school_state_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_school_train = np.array(tokens.texts_to_sequences(docs_school_state_train))\n",
    "sequences_school_val = np.array(tokens.texts_to_sequences(docs_school_state_val))\n",
    "sequences_school_test = np.array(tokens.texts_to_sequences(docs_school_state_test))\n",
    "vocab_size_school_state = len(tokens.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7eBcyZOai2q"
   },
   "outputs": [],
   "source": [
    "#Get the flattened output for school_state\n",
    "input_layer_school_state = Input(shape=(1,), name = \"encoded_school_state\")\n",
    "embedding_layer_school_state = Embedding(input_dim=vocab_size_school_state, output_dim=4, trainable=True)(input_layer_school_state)\n",
    "flatten_school_state = Flatten()(embedding_layer_school_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JeTeDK3Fai2w"
   },
   "source": [
    "## Categorical data: teacher_prefix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMiVgiHXai2x"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xtyjWIZWai2y"
   },
   "outputs": [],
   "source": [
    "#Get the teacher_prefix values\n",
    "docs_teacher_prefix_train=list(X_train.teacher_prefix.values)\n",
    "docs_teacher_prefix_val=list(X_val.teacher_prefix.values)\n",
    "docs_teacher_prefix_test=list(X_test.teacher_prefix.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_teacher_prefix_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_teacher_prefix_train = np.array(tokens.texts_to_sequences(docs_teacher_prefix_train))\n",
    "sequences_teacher_prefix_val = np.array(tokens.texts_to_sequences(docs_teacher_prefix_val))\n",
    "sequences_teacher_prefix_test = np.array(tokens.texts_to_sequences(docs_teacher_prefix_test))\n",
    "vocab_size_teacher_prefix = len(tokens.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m5yudtSNai21"
   },
   "outputs": [],
   "source": [
    "#Get the flattened output for teacher_prefix\n",
    "input_layer_teacher_prefix = Input(shape=(1,), name = \"teacher_prefix\")\n",
    "embedding_layer_teacher_prefix = Embedding(input_dim=vocab_size_teacher_prefix, output_dim=4, trainable=True)(input_layer_teacher_prefix)\n",
    "flatten_teacher_prefix = Flatten()(embedding_layer_teacher_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCkgeOPgai28"
   },
   "source": [
    "## Categorical data: project_grade_category "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PpyPFB7dai2-"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PB7nbUYMai2-"
   },
   "outputs": [],
   "source": [
    "#Get the project_grade_category values\n",
    "docs_project_grade_category_train=list(X_train.project_grade_category.values)\n",
    "docs_project_grade_category_val=list(X_val.project_grade_category.values)\n",
    "docs_project_grade_category_test=list(X_test.project_grade_category.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_project_grade_category_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_project_grade_category_train = tokens.texts_to_sequences(docs_project_grade_category_train)\n",
    "sequences_project_grade_category_val = tokens.texts_to_sequences(docs_project_grade_category_val)\n",
    "sequences_project_grade_category_test = tokens.texts_to_sequences(docs_project_grade_category_test)\n",
    "vocab_size_project_grade_category= len(tokens.word_index) + 1\n",
    "\n",
    "#Add padding\n",
    "padded_project_grade_category_train = pad_sequences(sequences_project_grade_category_train, maxlen=3, padding='post')\n",
    "padded_project_grade_category_val = pad_sequences(sequences_project_grade_category_val, maxlen=3, padding='post')\n",
    "padded_project_grade_category_test = pad_sequences(sequences_project_grade_category_test, maxlen=3, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOP-jcuLai3D"
   },
   "outputs": [],
   "source": [
    "#Get the flattened output for project_grade_category\n",
    "input_layer_project_grade = Input(shape=(3,), name = \"project_grade_category\")\n",
    "embedding_layer_project_grade = Embedding(input_dim=vocab_size_project_grade_category, output_dim=4, trainable=True)(input_layer_project_grade)\n",
    "flatten_project_grade = Flatten()(embedding_layer_project_grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6zMvdRNEai3F"
   },
   "source": [
    "## Categorical data: clean_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lah8psutai3M"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HCy7QHwai3N"
   },
   "outputs": [],
   "source": [
    "#Get the clean_categories values\n",
    "docs_clean_categories_train=list(X_train.clean_categories.values)\n",
    "docs_clean_categories_val=list(X_val.clean_categories.values)\n",
    "docs_clean_categories_test=list(X_test.clean_categories.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_clean_categories_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_clean_categories_train = tokens.texts_to_sequences(docs_clean_categories_train)\n",
    "sequences_clean_categories_val = tokens.texts_to_sequences(docs_clean_categories_val)\n",
    "sequences_clean_categories_test = tokens.texts_to_sequences(docs_clean_categories_test)\n",
    "vocab_size_clean_categories = len(tokens.word_index) + 1\n",
    "\n",
    "#Add padding\n",
    "padded_clean_categories_train = pad_sequences(sequences_clean_categories_train, maxlen=3, padding='post')\n",
    "padded_clean_categories_val = pad_sequences(sequences_clean_categories_val, maxlen=3, padding='post')\n",
    "padded_clean_categories_test = pad_sequences(sequences_clean_categories_test, maxlen=3, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHP5Z3KLai3R"
   },
   "outputs": [],
   "source": [
    "#Get the flattened output for clean_categories\n",
    "input_layer_clean_categories = Input(shape=(3,), name = \"clean_categories\")\n",
    "embedding_layer_clean_categories = Embedding(input_dim=vocab_size_clean_categories, output_dim=4, trainable=True)(input_layer_clean_categories)\n",
    "flatten_clean_categories = Flatten()(embedding_layer_clean_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1uyhOWnXai3V"
   },
   "source": [
    "## Categorical data: clean_subcategories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qzBXMoWhai3W"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbyDeZi8ai3Y"
   },
   "outputs": [],
   "source": [
    "#Get the clean_subcategories values\n",
    "docs_clean_subcategories_train=list(X_train.clean_subcategories.values)\n",
    "docs_clean_subcategories_val=list(X_val.clean_subcategories.values)\n",
    "docs_clean_subcategories_test=list(X_test.clean_subcategories.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_clean_subcategories_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_clean_subcategories_train = tokens.texts_to_sequences(docs_clean_subcategories_train)\n",
    "sequences_clean_subcategories_val = tokens.texts_to_sequences(docs_clean_subcategories_val)\n",
    "sequences_clean_subcategories_test = tokens.texts_to_sequences(docs_clean_subcategories_test)\n",
    "vocab_size_clean_subcategories = len(tokens.word_index) + 1\n",
    "\n",
    "padded_clean_subcategories_train = pad_sequences(sequences_clean_subcategories_train, maxlen=3, padding='post')\n",
    "padded_clean_subcategories_val = pad_sequences(sequences_clean_subcategories_val, maxlen=3, padding='post')\n",
    "padded_clean_subcategories_test = pad_sequences(sequences_clean_subcategories_test, maxlen=3, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PGjvMJd6ai3f"
   },
   "outputs": [],
   "source": [
    "#Get the flattened output for clean_subcategories\n",
    "input_layer_clean_subcategories = Input(shape=(3,), name = \"clean_subcategories\")\n",
    "embedding_layer_clean_subcategories = Embedding(input_dim=vocab_size_clean_subcategories, output_dim=4, trainable=True)(input_layer_clean_subcategories)\n",
    "flatten_clean_subcategories = Flatten()(embedding_layer_clean_subcategories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAk4eNjzai3s"
   },
   "source": [
    "## teacher_number_of_previously_posted_projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89QmnErNai3t"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLbAF8I1ai3t"
   },
   "outputs": [],
   "source": [
    "previous_projects_train = X_train.teacher_number_of_previously_posted_projects.values\n",
    "previous_projects_val = X_val.teacher_number_of_previously_posted_projects.values\n",
    "previous_projects_test = X_test.teacher_number_of_previously_posted_projects.values\n",
    "\n",
    "norm_previous_projects_train, normalizer = normalize_vars(previous_projects_train.reshape(1,-1))\n",
    "norm_previous_projects_val = normalizer.transform(previous_projects_val.reshape(1,-1))\n",
    "norm_previous_projects_test = normalizer.transform(previous_projects_test.reshape(1,-1))\n",
    "\n",
    "norm_previous_projects_train = norm_previous_projects_train.reshape(len(X_train),1)\n",
    "norm_previous_projects_val = norm_previous_projects_val.reshape(len(X_val),1)\n",
    "norm_previous_projects_test = norm_previous_projects_test.reshape(len(X_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TeVCF_XSai30"
   },
   "outputs": [],
   "source": [
    "#Input layer for teacher_number_of_previously_posted_projects\n",
    "input_layer_previous_projects = Input(shape=(1,), name = \"previous_projects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yvgXs6_Fai33"
   },
   "source": [
    "## price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zPUjf6JJai34"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pDdzZIuai34"
   },
   "outputs": [],
   "source": [
    "price_train = X_train.price.values\n",
    "price_val = X_val.price.values\n",
    "price_test = X_test.price.values\n",
    "\n",
    "norm_price_train, normalizer = normalize_vars(price_train.reshape(1,-1))\n",
    "norm_price_val = normalizer.transform(price_val.reshape(1,-1))\n",
    "norm_price_test = normalizer.transform(price_test.reshape(1,-1))\n",
    "\n",
    "norm_price_train = norm_price_train.reshape(len(X_train),1)\n",
    "norm_price_val = norm_price_val.reshape(len(X_val),1)\n",
    "norm_price_test = norm_price_test.reshape(len(X_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRid_bpPai36"
   },
   "outputs": [],
   "source": [
    "#Input layer for price\n",
    "input_layer_price = Input(shape=(1,), name = \"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2xwf3Onai39"
   },
   "source": [
    "## quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M9d5Jmv-ai3_"
   },
   "source": [
    "### Building train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__WrelVhai3_"
   },
   "outputs": [],
   "source": [
    "quantity_train = X_train.quantity.values\n",
    "quantity_val = X_val.quantity.values\n",
    "quantity_test = X_test.quantity.values\n",
    "\n",
    "norm_quantity_train, normalizer = normalize_vars(quantity_train.reshape(1,-1))\n",
    "norm_quantity_val = normalizer.transform(quantity_val.reshape(1,-1))\n",
    "norm_quantity_test = normalizer.transform(quantity_test.reshape(1,-1))\n",
    "\n",
    "norm_quantity_train = norm_quantity_train.reshape(len(X_train),1)\n",
    "norm_quantity_val = norm_quantity_val.reshape(len(X_val),1)\n",
    "norm_quantity_test = norm_quantity_test.reshape(len(X_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uzh7T0TCai4C"
   },
   "outputs": [],
   "source": [
    "#Input layer for quantity\n",
    "input_layer_quantity = Input(shape=(1,), name = \"quantity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation of the numerical layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0Zepipvai5t"
   },
   "outputs": [],
   "source": [
    "numerical_features_layers_concat = concatenate([input_layer_previous_projects, input_layer_price, input_layer_quantity])\n",
    "dense_layer_numerical = Dense(4, activation='relu',kernel_initializer='he_normal')(numerical_features_layers_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpEvAaSsai5w"
   },
   "source": [
    "### Concatenation of all the layers and building the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MK9gY0nrai5x"
   },
   "outputs": [],
   "source": [
    "del( X_train, X_val, y_train, y_val, X, y, project_data, file, embeddings_index, coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r20FB7o1ai50",
    "outputId": "a2cbd0dd-fe27-411d-8de2-224c644be937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "total_text_sequence (InputLayer (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 300, 300)     16691700    total_text_sequence[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "teacher_prefix (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoded_school_state (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "project_grade_category (InputLa (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_categories (InputLayer)   (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_subcategories (InputLayer (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "previous_projects (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "price (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "quantity (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300, 16)      20288       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 4)         24          teacher_prefix[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 4)         208         encoded_school_state[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 3, 4)         40          project_grade_category[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 3, 4)         64          clean_categories[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 3, 4)         152         clean_subcategories[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3)            0           previous_projects[0][0]          \n",
      "                                                                 price[0][0]                      \n",
      "                                                                 quantity[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4800)         0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 4)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4)            0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 12)           0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12)           0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 12)           0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            16          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 4848)         0           flatten_1[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_1 (Dense)           (None, 8)            38792       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8)            0           dense_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_2 (Dense)           (None, 4)            36          dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4)            0           dense_layer_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 1)            5           dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 16,751,325\n",
      "Trainable params: 59,625\n",
      "Non-trainable params: 16,691,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Merge all the layers according to the architecture diagram\n",
    "x = concatenate([flatten_total_text, flatten_teacher_prefix, flatten_school_state, flatten_project_grade, flatten_clean_categories, flatten_clean_subcategories, dense_layer_numerical])\n",
    "x = Dense(8, activation='relu',kernel_initializer='he_normal', name='dense_layer_1')(x)\n",
    "x = Dropout(0.4, name='dropout_1')(x)\n",
    "x = Dense(4, activation='relu',kernel_initializer='he_normal',name='dense_layer_2')(x)\n",
    "x = Dropout(0.4, name='dropout_2')(x)\n",
    "output_layer = Dense(1, activation='sigmoid', name='output_layer')(x)\n",
    "\n",
    "# Final model 2\n",
    "model = Model(inputs=[input_layer_total_text,input_layer_teacher_prefix,input_layer_school_state,input_layer_project_grade,input_layer_clean_categories,\n",
    "                      input_layer_clean_subcategories,input_layer_previous_projects,input_layer_price,input_layer_quantity], outputs=[output_layer])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQXlQQI0ai55"
   },
   "source": [
    "### Compiling the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vy2EvXYJai56",
    "outputId": "6eee668d-d4e8-4ad1-d5cc-6d7bc120f506"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uc5vZL5Zai6F",
    "outputId": "c627315a-579a-4f71-af30-9f612f8d71f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87398 samples, validate on 21850 samples\n",
      "Epoch 1/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.4946 - roc_auc: 0.5318 - val_loss: 0.4012 - val_roc_auc: 0.5635\n",
      "Epoch 2/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.4578 - roc_auc: 0.5802 - val_loss: 0.3953 - val_roc_auc: 0.5931\n",
      "Epoch 3/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.4391 - roc_auc: 0.6024 - val_loss: 0.3973 - val_roc_auc: 0.6093\n",
      "Epoch 4/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.4268 - roc_auc: 0.6152 - val_loss: 0.3927 - val_roc_auc: 0.6198\n",
      "Epoch 5/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.4192 - roc_auc: 0.6240 - val_loss: 0.3882 - val_roc_auc: 0.6276\n",
      "Epoch 6/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.4151 - roc_auc: 0.6307 - val_loss: 0.3861 - val_roc_auc: 0.6338\n",
      "Epoch 7/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.4097 - roc_auc: 0.6369 - val_loss: 0.3864 - val_roc_auc: 0.6399\n",
      "Epoch 8/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.4031 - roc_auc: 0.6431 - val_loss: 0.3872 - val_roc_auc: 0.6462\n",
      "Epoch 9/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3997 - roc_auc: 0.6494 - val_loss: 0.3881 - val_roc_auc: 0.6523\n",
      "Epoch 10/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3940 - roc_auc: 0.6554 - val_loss: 0.3898 - val_roc_auc: 0.6582\n",
      "Epoch 11/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3888 - roc_auc: 0.6613 - val_loss: 0.3947 - val_roc_auc: 0.6641\n",
      "Epoch 12/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3862 - roc_auc: 0.6670 - val_loss: 0.3944 - val_roc_auc: 0.6695\n",
      "Epoch 13/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3802 - roc_auc: 0.6722 - val_loss: 0.3985 - val_roc_auc: 0.6748\n",
      "Epoch 14/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3755 - roc_auc: 0.6776 - val_loss: 0.4014 - val_roc_auc: 0.6800\n",
      "Epoch 15/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3733 - roc_auc: 0.6825 - val_loss: 0.4045 - val_roc_auc: 0.6848\n",
      "Epoch 16/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3675 - roc_auc: 0.6874 - val_loss: 0.4095 - val_roc_auc: 0.6895\n",
      "Epoch 17/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3644 - roc_auc: 0.6918 - val_loss: 0.4168 - val_roc_auc: 0.6938\n",
      "Epoch 18/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3612 - roc_auc: 0.6961 - val_loss: 0.4200 - val_roc_auc: 0.6980\n",
      "Epoch 19/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3569 - roc_auc: 0.7002 - val_loss: 0.4226 - val_roc_auc: 0.7020\n",
      "Epoch 20/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3531 - roc_auc: 0.7042 - val_loss: 0.4348 - val_roc_auc: 0.7059\n",
      "Epoch 21/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3509 - roc_auc: 0.7079 - val_loss: 0.4503 - val_roc_auc: 0.7096\n",
      "Epoch 22/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3466 - roc_auc: 0.7115 - val_loss: 0.4373 - val_roc_auc: 0.7132\n",
      "Epoch 23/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3444 - roc_auc: 0.7149 - val_loss: 0.4502 - val_roc_auc: 0.7166\n",
      "Epoch 24/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3387 - roc_auc: 0.7184 - val_loss: 0.4689 - val_roc_auc: 0.7200\n",
      "Epoch 25/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3353 - roc_auc: 0.7217 - val_loss: 0.4596 - val_roc_auc: 0.7233\n",
      "Epoch 26/50\n",
      "87398/87398 [==============================] - 128s 1ms/step - loss: 0.3339 - roc_auc: 0.7249 - val_loss: 0.4969 - val_roc_auc: 0.7263\n",
      "Epoch 27/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3303 - roc_auc: 0.7280 - val_loss: 0.4727 - val_roc_auc: 0.7294\n",
      "Epoch 28/50\n",
      "87398/87398 [==============================] - 128s 1ms/step - loss: 0.3268 - roc_auc: 0.7309 - val_loss: 0.5163 - val_roc_auc: 0.7323\n",
      "Epoch 29/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3236 - roc_auc: 0.7338 - val_loss: 0.5002 - val_roc_auc: 0.7352\n",
      "Epoch 30/50\n",
      "87398/87398 [==============================] - 130s 1ms/step - loss: 0.3194 - roc_auc: 0.7366 - val_loss: 0.5384 - val_roc_auc: 0.7379\n",
      "Epoch 31/50\n",
      "87398/87398 [==============================] - 130s 1ms/step - loss: 0.3190 - roc_auc: 0.7393 - val_loss: 0.5489 - val_roc_auc: 0.7405\n",
      "Epoch 32/50\n",
      "87398/87398 [==============================] - 135s 2ms/step - loss: 0.3144 - roc_auc: 0.7419 - val_loss: 0.5324 - val_roc_auc: 0.7431\n",
      "Epoch 33/50\n",
      "87398/87398 [==============================] - 135s 2ms/step - loss: 0.3136 - roc_auc: 0.7444 - val_loss: 0.5974 - val_roc_auc: 0.7455\n",
      "Epoch 34/50\n",
      "87398/87398 [==============================] - 131s 2ms/step - loss: 0.3110 - roc_auc: 0.7467 - val_loss: 0.5594 - val_roc_auc: 0.7478\n",
      "Epoch 35/50\n",
      "87398/87398 [==============================] - 132s 2ms/step - loss: 0.3057 - roc_auc: 0.7491 - val_loss: 0.5782 - val_roc_auc: 0.7502\n",
      "Epoch 36/50\n",
      "87398/87398 [==============================] - 130s 1ms/step - loss: 0.3042 - roc_auc: 0.7514 - val_loss: 0.5782 - val_roc_auc: 0.7524\n",
      "Epoch 37/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3032 - roc_auc: 0.7536 - val_loss: 0.6201 - val_roc_auc: 0.7545\n",
      "Epoch 38/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.3018 - roc_auc: 0.7556 - val_loss: 0.6250 - val_roc_auc: 0.7565\n",
      "Epoch 39/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.2981 - roc_auc: 0.7576 - val_loss: 0.5957 - val_roc_auc: 0.7586\n",
      "Epoch 40/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.2939 - roc_auc: 0.7596 - val_loss: 0.6759 - val_roc_auc: 0.7606\n",
      "Epoch 41/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.2919 - roc_auc: 0.7616 - val_loss: 0.6783 - val_roc_auc: 0.7626\n",
      "Epoch 42/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.2893 - roc_auc: 0.7636 - val_loss: 0.6458 - val_roc_auc: 0.7645\n",
      "Epoch 43/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.2882 - roc_auc: 0.7655 - val_loss: 0.6851 - val_roc_auc: 0.7663\n",
      "Epoch 44/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.2847 - roc_auc: 0.7673 - val_loss: 0.7415 - val_roc_auc: 0.7682\n",
      "Epoch 45/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.2818 - roc_auc: 0.7692 - val_loss: 0.6992 - val_roc_auc: 0.7700\n",
      "Epoch 46/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.2813 - roc_auc: 0.7710 - val_loss: 0.7781 - val_roc_auc: 0.7717\n",
      "Epoch 47/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.2803 - roc_auc: 0.7726 - val_loss: 0.7276 - val_roc_auc: 0.7734\n",
      "Epoch 48/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.2778 - roc_auc: 0.7743 - val_loss: 0.7539 - val_roc_auc: 0.7750\n",
      "Epoch 49/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.2750 - roc_auc: 0.7759 - val_loss: 0.7324 - val_roc_auc: 0.7766\n",
      "Epoch 50/50\n",
      "87398/87398 [==============================] - 127s 1ms/step - loss: 0.2770 - roc_auc: 0.7774 - val_loss: 0.7406 - val_roc_auc: 0.7781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f36a658c9e8>"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[roc_auc])\n",
    "model.fit(x=[padded_text_train,sequences_teacher_prefix_train,sequences_school_train,padded_project_grade_category_train,padded_clean_categories_train,padded_clean_subcategories_train,\n",
    "             norm_previous_projects_train,norm_price_train,norm_quantity_train], \n",
    "          y=[labels_train],\n",
    "          validation_data=([padded_text_val,sequences_teacher_prefix_val,sequences_school_val,padded_project_grade_category_val,padded_clean_categories_val,padded_clean_subcategories_val,\n",
    "                            norm_previous_projects_val,norm_price_val,norm_quantity_val],[labels_val]),\n",
    "          epochs=30, \n",
    "          batch_size=1024, \n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWNSYwcrai6Q",
    "outputId": "a37763e9-b52e-4265-d22c-fe895f9c7a75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.13.1 at http://saugata:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Qw-7qZ4ai6o"
   },
   "source": [
    "<img src='Model2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on unseen test data:  0.7164852592744682\n",
      "Accuracy on unseen test data:  0.848512585812357\n"
     ]
    }
   ],
   "source": [
    "test_data=[padded_text_test,sequences_teacher_prefix_test,sequences_school_test,padded_project_grade_category_test,\n",
    "                  padded_clean_categories_test,padded_clean_subcategories_test,norm_previous_projects_test,norm_price_test,norm_quantity_test]\n",
    "\n",
    "#Test AUC\n",
    "y_pred= model.predict(test_data)\n",
    "print(\"AUC on unseen test data: \",roc_auc_score(y_test,y_pred))\n",
    "\n",
    "#Test Accuracy\n",
    "pred = [y_pred[i][0] for i in range(y_pred.shape[0])]\n",
    "class_pred = [1 if i>0.5 else 0 for i in pred]\n",
    "print(\"Accuracy on unseen test data: \",accuracy_score(y_test,class_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "model.save(\"model1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s05I3kW2ai6p"
   },
   "source": [
    "# Model-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYyB8hK5ai6q"
   },
   "source": [
    "<img src='https://i.imgur.com/fkQ8nGo.png'>\n",
    "ref: https://i.imgur.com/fkQ8nGo.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVMGnROjai6r"
   },
   "source": [
    "\n",
    "- __input_seq_total_text_data__: <br>\n",
    "<pre>\n",
    "    . Use text column('essay'), and use the Embedding layer to get word vectors. <br>\n",
    "    . Use given predefined glove word vectors, don't train any word vectors. <br>\n",
    "    . Use LSTM that is given above, get the LSTM output and Flatten that output. <br>\n",
    "    . You are free to preprocess the input text as you needed. <br>\n",
    "</pre>\n",
    "- __Other_than_text_data__:<br>\n",
    "<pre>\n",
    "    . Convert all your Categorical values to onehot coded and then concatenate all these onehot vectors <br>\n",
    "    . Neumerical values and use <a href='https://keras.io/getting-started/sequential-model-guide/#sequence-classification-with-1d-convolutions'>CNN1D</a> as shown in above figure. <br>\n",
    "    . You are free to choose all CNN parameters like kernel sizes, stride.<br>\n",
    "    \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UC7UWUs7ai6t"
   },
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7NlCgRw2ai6v"
   },
   "outputs": [],
   "source": [
    "project_data = pd.read_csv(\"processed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOB144u6ai6z"
   },
   "source": [
    "## 2. Splitting the original data into train and test data in 80:20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whzkMR6zai60",
    "outputId": "43425bbe-6271-4c22-cd02-6035d037fc1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in train data:  87398\n",
      "Number of points in validation data:  17480\n",
      "Number of points in test data:  4370\n"
     ]
    }
   ],
   "source": [
    "#Taking the target and predictor variables into separate variables\n",
    "y = project_data[\"project_is_approved\"] #target variables\n",
    "X = project_data.drop(['project_is_approved'], axis=1) #predictor variables\n",
    "\n",
    "#Split the dataset into train and val dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.20, random_state=1, stratify=y_test)\n",
    "\n",
    "#Display basic information after splitting the data\n",
    "print(\"Number of points in train data: \",X_train.shape[0])\n",
    "print(\"Number of points in validation data: \",X_val.shape[0])\n",
    "print(\"Number of points in test data: \",X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KL5a1UXOai65"
   },
   "source": [
    "## 3. Tokenizing Total Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FFmK5N1ai2a"
   },
   "source": [
    "### Building train, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJuU959Rai2b"
   },
   "outputs": [],
   "source": [
    "#Get the total_text values in list\n",
    "docs_text_train=list(X_train.total_text.values)\n",
    "docs_text_val=list(X_val.total_text.values)\n",
    "docs_text_test=list(X_test.total_text.values)\n",
    "labels_train=np.array(y_train)\n",
    "labels_val=np.array(y_val)\n",
    "labels_test=np.array(y_test)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_text_train)\n",
    "\n",
    "#Convert the texts to sequences using the tokenizer\n",
    "sequences_text_train = tokens.texts_to_sequences(docs_text_train)\n",
    "sequences_text_val = tokens.texts_to_sequences(docs_text_val)\n",
    "sequences_text_test = tokens.texts_to_sequences(docs_text_test)\n",
    "vocab_size_text = len(tokens.word_index) + 1\n",
    "\n",
    "#Add padding\n",
    "padded_text_train = pad_sequences(sequences_text_train, maxlen=300, padding='post')\n",
    "padded_text_val = pad_sequences(sequences_text_val, maxlen=300, padding='post')\n",
    "padded_text_test = pad_sequences(sequences_text_test, maxlen=300, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9AV1Uk3ai7C",
    "outputId": "f8d15943-467f-445b-c856-5ad1cc92422b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55638/55638 [00:00<00:00, 302291.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "file = open('glove.6B.300d.txt')\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "file.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#Create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size_text, 300))\n",
    "for word, i in tqdm(tokens.word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector #embedding_matrix.shape: (9049, 300)\n",
    "\n",
    "print(len(embedding_matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_-1YjoFai7G"
   },
   "outputs": [],
   "source": [
    "#Get the flattened LSTM output for input text\n",
    "input_layer_total_text = Input(shape=(300,), name = \"total_text_sequence\")\n",
    "embedding_layer_total_text = Embedding(input_dim=vocab_size_text, output_dim=300, weights=[embedding_matrix], trainable=False)(input_layer_total_text)\n",
    "lstm_total_text  = LSTM(16, activation=\"relu\", return_sequences=True)(embedding_layer_total_text)\n",
    "flatten_lstm_out = Flatten()(lstm_total_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHePYqbzai7I"
   },
   "source": [
    "## 4. One hot encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Xk4kdr5ai7I"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GW5xi-oRai7N"
   },
   "outputs": [],
   "source": [
    "#School state\n",
    "encoder=OneHotEncoder().fit(X_train['school_state'].values.reshape(-1,1))\n",
    "enc_school_state_train=encoder.transform(X_train['school_state'].values.reshape(-1,1))\n",
    "enc_school_state_val=encoder.transform(X_val['school_state'].values.reshape(-1, 1))\n",
    "enc_school_state_test=encoder.transform(X_test['school_state'].values.reshape(-1, 1))\n",
    "\n",
    "#Teacher prefix\n",
    "encoder=OneHotEncoder().fit(X_train['teacher_prefix'].values.reshape(-1,1))\n",
    "enc_teacher_prefix_train=encoder.transform(X_train['teacher_prefix'].values.reshape(-1,1))\n",
    "enc_teacher_prefix_val=encoder.transform(X_val['teacher_prefix'].values.reshape(-1, 1))\n",
    "enc_teacher_prefix_test=encoder.transform(X_test['teacher_prefix'].values.reshape(-1, 1))\n",
    "\n",
    "#project_grade_category\n",
    "encoder=OneHotEncoder().fit(X_train['project_grade_category'].values.reshape(-1,1))\n",
    "enc_project_grade_category_train=encoder.transform(X_train['project_grade_category'].values.reshape(-1,1))\n",
    "enc_project_grade_category_val=encoder.transform(X_val['project_grade_category'].values.reshape(-1, 1))\n",
    "enc_project_grade_category_test=encoder.transform(X_test['project_grade_category'].values.reshape(-1, 1))\n",
    "\n",
    "#clean_categories\n",
    "encoder=CountVectorizer(binary=True).fit(X_train['clean_categories'])\n",
    "enc_clean_categories_category_train=encoder.transform(X_train['clean_categories'])\n",
    "enc_clean_categories_category_val=encoder.transform(X_val['clean_categories'])\n",
    "enc_clean_categories_category_test=encoder.transform(X_test['clean_categories'])\n",
    "\n",
    "#clean_subcategories\n",
    "encoder=CountVectorizer(binary=True).fit(X_train['clean_subcategories'])\n",
    "enc_clean_subcategories_train=encoder.transform(X_train['clean_subcategories'])\n",
    "enc_clean_subcategories_val=encoder.transform(X_val['clean_subcategories'])\n",
    "enc_clean_subcategories_test=encoder.transform(X_test['clean_subcategories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jU-bgKZcai7O"
   },
   "source": [
    "## 5. Normalizing numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LBuCd46Lai7Q"
   },
   "outputs": [],
   "source": [
    "#teacher_number_of_previously_posted_projects\n",
    "previous_projects_train = X_train.teacher_number_of_previously_posted_projects.values\n",
    "previous_projects_val = X_val.teacher_number_of_previously_posted_projects.values\n",
    "previous_projects_test = X_test.teacher_number_of_previously_posted_projects.values\n",
    "\n",
    "norm_previous_projects_train, normalizer = normalize_vars(previous_projects_train.reshape(1,-1))\n",
    "norm_previous_projects_val = normalizer.transform(previous_projects_val.reshape(1,-1))\n",
    "norm_previous_projects_test = normalizer.transform(previous_projects_test.reshape(1,-1))\n",
    "\n",
    "norm_previous_projects_train = norm_previous_projects_train.reshape(len(X_train),1)\n",
    "norm_previous_projects_val = norm_previous_projects_val.reshape(len(X_val),1)\n",
    "norm_previous_projects_test = norm_previous_projects_test.reshape(len(X_test),1)\n",
    "\n",
    "#price\n",
    "price_train = X_train.price.values\n",
    "price_val = X_val.price.values\n",
    "price_test = X_test.price.values\n",
    "\n",
    "norm_price_train, normalizer = normalize_vars(price_train.reshape(1,-1))\n",
    "norm_price_val = normalizer.transform(price_val.reshape(1,-1))\n",
    "norm_price_test = normalizer.transform(price_test.reshape(1,-1))\n",
    "\n",
    "norm_price_train = norm_price_train.reshape(len(X_train),1)\n",
    "norm_price_val = norm_price_val.reshape(len(X_val),1)\n",
    "norm_price_test = norm_price_test.reshape(len(X_test),1)\n",
    "\n",
    "#quantity\n",
    "quantity_train = X_train.quantity.values\n",
    "quantity_val = X_val.quantity.values\n",
    "quantity_test = X_test.quantity.values\n",
    "\n",
    "norm_quantity_train, normalizer = normalize_vars(quantity_train.reshape(1,-1))\n",
    "norm_quantity_val = normalizer.transform(quantity_val.reshape(1,-1))\n",
    "norm_quantity_test = normalizer.transform(quantity_test.reshape(1,-1))\n",
    "\n",
    "norm_quantity_train = norm_quantity_train.reshape(len(X_train),1)\n",
    "norm_quantity_val = norm_quantity_val.reshape(len(X_val),1)\n",
    "norm_quantity_test = norm_quantity_test.reshape(len(X_test),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Y8_5PR7ai7X"
   },
   "source": [
    "## 5. Stacking the numerical and categorical vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBlDzYtlai7Y"
   },
   "outputs": [],
   "source": [
    "stacked_vectors_train = hstack((enc_school_state_train,enc_teacher_prefix_train,enc_project_grade_category_train,enc_clean_categories_category_train,enc_clean_subcategories_train,\n",
    "                                norm_previous_projects_train,norm_price_train,norm_quantity_train))\n",
    "\n",
    "stacked_vectors_val = hstack((enc_school_state_val,enc_teacher_prefix_val,enc_project_grade_category_val,enc_clean_categories_category_val,enc_clean_subcategories_val,\n",
    "                                norm_previous_projects_val,norm_price_val,norm_quantity_val))\n",
    "\n",
    "stacked_vectors_test = hstack((enc_school_state_test,enc_teacher_prefix_test,enc_project_grade_category_test,enc_clean_categories_category_test,enc_clean_subcategories_test,\n",
    "                                norm_previous_projects_test,norm_price_test,norm_quantity_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdAQeT5Fai7a"
   },
   "outputs": [],
   "source": [
    "encoded_df_train = np.expand_dims(pd.DataFrame(stacked_vectors_train.todense()), axis=2)\n",
    "encoded_df_val = np.expand_dims(pd.DataFrame(stacked_vectors_val.todense()), axis=2)\n",
    "encoded_df_test = np.expand_dims(pd.DataFrame(stacked_vectors_test.todense()), axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "69CM8Gjvai7b"
   },
   "source": [
    "## 6. Defining the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mEXTOO6gai7d"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling2D, Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "conv_input = Input(shape=(stacked_vectors_train.shape[1],1), name=\"non_text_data_layer\")\n",
    "x = Conv1D(filters=8, kernel_size=4, activation='relu', kernel_initializer='he_normal',name='conv_layer_1')(conv_input)\n",
    "x = Conv1D(filters=8, kernel_size=4, activation='relu', kernel_initializer='he_normal',name='conv_layer_2')(x)\n",
    "flatten_conv_output = Flatten()(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlWVgdE7ai7e"
   },
   "outputs": [],
   "source": [
    "del( X_train, X_val, y_train, y_val, X, y, project_data, file, embeddings_index, coefs, stacked_vectors_val, stacked_vectors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-0VTSWIai7g",
    "outputId": "68939f34-673a-470f-f72b-a0f95710412f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "total_text_sequence (InputLayer (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "non_text_data_layer (InputLayer (None, 102, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 300, 300)     16691700    total_text_sequence[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_layer_1 (Conv1D)           (None, 99, 8)        40          non_text_data_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 300, 16)      20288       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_layer_2 (Conv1D)           (None, 96, 8)        264         conv_layer_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4800)         0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 768)          0           conv_layer_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 5568)         0           flatten_2[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_1 (Dense)           (None, 32)           178208      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           dense_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_2 (Dense)           (None, 64)           2112        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64)           0           dense_layer_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_3 (Dense)           (None, 128)          8320        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            129         dense_layer_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 16,901,061\n",
      "Trainable params: 209,361\n",
      "Non-trainable params: 16,691,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout\n",
    "\n",
    "x = concatenate([flatten_lstm_out, flatten_conv_output])\n",
    "\n",
    "x = Dense(32, activation='relu', kernel_initializer='he_normal',name='dense_layer_1')(x)\n",
    "x = Dropout(0.6)(x)\n",
    "x = Dense(64, activation='relu', kernel_initializer='he_normal',name='dense_layer_2')(x)\n",
    "x = Dropout(0.6)(x)\n",
    "x = Dense(128, activation='relu', kernel_initializer='he_normal',name='dense_layer_3')(x)\n",
    "\n",
    "output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "model = Model(inputs=[input_layer_total_text,conv_input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w8R-zqQbai7j"
   },
   "source": [
    "## 7. Compiling the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4yWHdXVFai7l",
    "outputId": "3c08d9c8-8d40-46f7-df2c-2fee02a67f7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:526: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:788: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 87398 samples, validate on 21850 samples\n",
      "Epoch 1/50\n",
      "87398/87398 [==============================] - 131s 2ms/step - loss: 0.4499 - roc_auc: 0.5471 - val_loss: 0.4042 - val_roc_auc: 0.5973\n",
      "Epoch 2/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.4035 - roc_auc: 0.6227 - val_loss: 0.4304 - val_roc_auc: 0.6372\n",
      "Epoch 3/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3943 - roc_auc: 0.6473 - val_loss: 0.4397 - val_roc_auc: 0.6553\n",
      "Epoch 4/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3882 - roc_auc: 0.6615 - val_loss: 0.4159 - val_roc_auc: 0.6684\n",
      "Epoch 5/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3830 - roc_auc: 0.6738 - val_loss: 0.4171 - val_roc_auc: 0.6791\n",
      "Epoch 6/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3783 - roc_auc: 0.6837 - val_loss: 0.4164 - val_roc_auc: 0.6875\n",
      "Epoch 7/50\n",
      "87398/87398 [==============================] - 128s 1ms/step - loss: 0.3735 - roc_auc: 0.6912 - val_loss: 0.4215 - val_roc_auc: 0.6943\n",
      "Epoch 8/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3707 - roc_auc: 0.6973 - val_loss: 0.4314 - val_roc_auc: 0.6995\n",
      "Epoch 9/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3665 - roc_auc: 0.7021 - val_loss: 0.4127 - val_roc_auc: 0.7047\n",
      "Epoch 10/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3626 - roc_auc: 0.7073 - val_loss: 0.4340 - val_roc_auc: 0.7093\n",
      "Epoch 11/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3594 - roc_auc: 0.7113 - val_loss: 0.4338 - val_roc_auc: 0.7131\n",
      "Epoch 12/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3555 - roc_auc: 0.7152 - val_loss: 0.4217 - val_roc_auc: 0.7171\n",
      "Epoch 13/50\n",
      "87398/87398 [==============================] - 128s 1ms/step - loss: 0.3512 - roc_auc: 0.7194 - val_loss: 0.4349 - val_roc_auc: 0.7210\n",
      "Epoch 14/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3490 - roc_auc: 0.7229 - val_loss: 0.4271 - val_roc_auc: 0.7246\n",
      "Epoch 15/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3462 - roc_auc: 0.7264 - val_loss: 0.4642 - val_roc_auc: 0.7275\n",
      "Epoch 16/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3414 - roc_auc: 0.7290 - val_loss: 0.4253 - val_roc_auc: 0.7306\n",
      "Epoch 17/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3398 - roc_auc: 0.7323 - val_loss: 0.4185 - val_roc_auc: 0.7339\n",
      "Epoch 18/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3383 - roc_auc: 0.7356 - val_loss: 0.4143 - val_roc_auc: 0.7370\n",
      "Epoch 19/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3355 - roc_auc: 0.7387 - val_loss: 0.4123 - val_roc_auc: 0.7401\n",
      "Epoch 20/50\n",
      "87398/87398 [==============================] - 128s 1ms/step - loss: 0.3299 - roc_auc: 0.7418 - val_loss: 0.4153 - val_roc_auc: 0.7433\n",
      "Epoch 21/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3290 - roc_auc: 0.7449 - val_loss: 0.4004 - val_roc_auc: 0.7463\n",
      "Epoch 22/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3255 - roc_auc: 0.7478 - val_loss: 0.4155 - val_roc_auc: 0.7492\n",
      "Epoch 23/50\n",
      "87398/87398 [==============================] - 135s 2ms/step - loss: 0.3245 - roc_auc: 0.7506 - val_loss: 0.3949 - val_roc_auc: 0.7519\n",
      "Epoch 24/50\n",
      "87398/87398 [==============================] - 128s 1ms/step - loss: 0.3223 - roc_auc: 0.7534 - val_loss: 0.4030 - val_roc_auc: 0.7547\n",
      "Epoch 25/50\n",
      "87398/87398 [==============================] - 128s 1ms/step - loss: 0.3218 - roc_auc: 0.7560 - val_loss: 0.4130 - val_roc_auc: 0.7571\n",
      "Epoch 26/50\n",
      "87398/87398 [==============================] - 131s 1ms/step - loss: 0.3198 - roc_auc: 0.7583 - val_loss: 0.3946 - val_roc_auc: 0.7594\n",
      "Epoch 27/50\n",
      "87398/87398 [==============================] - 136s 2ms/step - loss: 0.3166 - roc_auc: 0.7607 - val_loss: 0.4026 - val_roc_auc: 0.7618\n",
      "Epoch 28/50\n",
      "87398/87398 [==============================] - 131s 1ms/step - loss: 0.3153 - roc_auc: 0.7630 - val_loss: 0.4436 - val_roc_auc: 0.7638\n",
      "Epoch 29/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3132 - roc_auc: 0.7648 - val_loss: 0.4283 - val_roc_auc: 0.7657\n",
      "Epoch 30/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3126 - roc_auc: 0.7668 - val_loss: 0.4204 - val_roc_auc: 0.7677\n",
      "Epoch 31/50\n",
      "87398/87398 [==============================] - 132s 2ms/step - loss: 0.3119 - roc_auc: 0.7687 - val_loss: 0.4072 - val_roc_auc: 0.7696\n",
      "Epoch 32/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3092 - roc_auc: 0.7706 - val_loss: 0.4066 - val_roc_auc: 0.7715\n",
      "Epoch 33/50\n",
      "87398/87398 [==============================] - 128s 1ms/step - loss: 0.3081 - roc_auc: 0.7725 - val_loss: 0.4185 - val_roc_auc: 0.7733\n",
      "Epoch 34/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3085 - roc_auc: 0.7742 - val_loss: 0.4077 - val_roc_auc: 0.7749\n",
      "Epoch 35/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3088 - roc_auc: 0.7758 - val_loss: 0.4060 - val_roc_auc: 0.7765\n",
      "Epoch 36/50\n",
      "87398/87398 [==============================] - 129s 1ms/step - loss: 0.3043 - roc_auc: 0.7774 - val_loss: 0.4080 - val_roc_auc: 0.7782\n",
      "Epoch 37/50\n",
      "87398/87398 [==============================] - 128s 1ms/step - loss: 0.3030 - roc_auc: 0.7791 - val_loss: 0.4205 - val_roc_auc: 0.7798\n",
      "Epoch 38/50\n",
      "87398/87398 [==============================] - 134s 2ms/step - loss: 0.3034 - roc_auc: 0.7806 - val_loss: 0.4177 - val_roc_auc: 0.7812\n",
      "Epoch 39/50\n",
      "87398/87398 [==============================] - 135s 2ms/step - loss: 0.3018 - roc_auc: 0.7820 - val_loss: 0.4116 - val_roc_auc: 0.7826\n",
      "Epoch 40/50\n",
      "87398/87398 [==============================] - 145s 2ms/step - loss: 0.3011 - roc_auc: 0.7834 - val_loss: 0.4070 - val_roc_auc: 0.7840\n",
      "Epoch 41/50\n",
      "87398/87398 [==============================] - 148s 2ms/step - loss: 0.3016 - roc_auc: 0.7847 - val_loss: 0.4286 - val_roc_auc: 0.7853\n",
      "Epoch 42/50\n",
      "87398/87398 [==============================] - 142s 2ms/step - loss: 0.3008 - roc_auc: 0.7860 - val_loss: 0.4113 - val_roc_auc: 0.7866\n",
      "Epoch 43/50\n",
      "87398/87398 [==============================] - 146s 2ms/step - loss: 0.3013 - roc_auc: 0.7872 - val_loss: 0.4101 - val_roc_auc: 0.7877\n",
      "Epoch 44/50\n",
      "87398/87398 [==============================] - 141s 2ms/step - loss: 0.2989 - roc_auc: 0.7884 - val_loss: 0.4096 - val_roc_auc: 0.7889\n",
      "Epoch 45/50\n",
      "87398/87398 [==============================] - 142s 2ms/step - loss: 0.2984 - roc_auc: 0.7895 - val_loss: 0.4206 - val_roc_auc: 0.7900\n",
      "Epoch 46/50\n",
      "87398/87398 [==============================] - 142s 2ms/step - loss: 0.2977 - roc_auc: 0.7906 - val_loss: 0.4115 - val_roc_auc: 0.7911\n",
      "Epoch 47/50\n",
      "87398/87398 [==============================] - 143s 2ms/step - loss: 0.2967 - roc_auc: 0.7917 - val_loss: 0.4190 - val_roc_auc: 0.7922\n",
      "Epoch 48/50\n",
      "87398/87398 [==============================] - 143s 2ms/step - loss: 0.2958 - roc_auc: 0.7928 - val_loss: 0.4200 - val_roc_auc: 0.7933\n",
      "Epoch 49/50\n",
      "87398/87398 [==============================] - 144s 2ms/step - loss: 0.2954 - roc_auc: 0.7938 - val_loss: 0.4263 - val_roc_auc: 0.7942\n",
      "Epoch 50/50\n",
      "87398/87398 [==============================] - 144s 2ms/step - loss: 0.2976 - roc_auc: 0.7947 - val_loss: 0.4076 - val_roc_auc: 0.7951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe6952e6eb8>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[roc_auc])\n",
    "model.fit(x=[padded_text_train, encoded_df_train], \n",
    "          y=[labels_train],\n",
    "          epochs=50, \n",
    "          batch_size=1024,\n",
    "          validation_data=([padded_text_val, encoded_df_val],[labels_val]),\n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qy4AI8Xai7n",
    "outputId": "fc3fc0c6-2cf2-483b-d8bc-387fcb58fbe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.13.1 at http://saugata:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7Jteb8Vai7p"
   },
   "source": [
    "<img src='Model3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBk2K-eYai7q"
   },
   "source": [
    "# Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nzt2Vluxai7r",
    "outputId": "3c36e861-1101-4a7a-ac57-4f610e8da1bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------+--------------+\n",
      "| Model No | Trained for Epochs | Train ROC-AUC | Test ROC-AUC |\n",
      "+----------+--------------------+---------------+--------------+\n",
      "| Model 1  |         40         |     0.7344    |    0.7352    |\n",
      "| Model 2  |         50         |     0.7774    |    0.7781    |\n",
      "| Model 3  |         50         |     0.7947    |    0.7951    |\n",
      "+----------+--------------------+---------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "table =PrettyTable()\n",
    "table.field_names = [\"Model No\", \"Trained for Epochs\", \"Train ROC-AUC\", \"Validation ROC-AUC\", \"Test ROC-AUC\"]\n",
    "table.add_row([\"Model 1\",40,0.7344,0.7352])\n",
    "table.add_row([\"Model 2\",50,0.7774,0.7781])\n",
    "table.add_row([\"Model 3\",50,0.7947,0.7951])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVJ15W_Lai7t"
   },
   "source": [
    "# Brief Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y00Y7_HQai7t"
   },
   "source": [
    "1. Amongst all the three models, the 3rd model with 1D CNN layers seems to perform the best as we have got the maximum value of ROC-AUC for it.\n",
    "2. I first tried without proper weight initialization in the hidden layers, but this was resulting in a massive overfitting and the models were trained way-wardly. The test roc auc was always 7-10% more than the validation roc-auc.There was also an issue of exploding gradients after training the models for longer epochs. The problem of exploding gradients seems to have been resolved on using proper initialization techniques.\n",
    "3. Using RSM Prop with proper weight initialization was also resulting in exploding gradients for Model 1. Changing the optimizer to adam has changed this problem.\n",
    "4. Got the loss curves and score curve using Tensorboard. \n",
    "5. Used a custome metric function for training the model with a custom roc-auc score.\n",
    "6. For TFIDF analysis, I first tried with the 25 percentile threshold for tfidf scores and the model performed very poorly despiting trying my best to optimize it. The lowest threshold I have considered for this assignment is 6.5 approximately.\n",
    "7. By far, Model 2 has given us the best curve in terms of vaidation loss. The curve shows no overfitting, and we also have obtained close to 0.78 AUC score.\n",
    "8. Model 3 has given us the best value of ROC-AUC - just under 0.80 for training the model for 50 epochs. \n",
    "9. Used this blog as a reference for doing this assignment: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6tPx_wGjai0K",
    "fPxsuz1Bai0U",
    "gw__QsaLai0f",
    "khlhBO5Sai0n",
    "QKBe8NZdai0v",
    "Eg9Y7ZZpai1H",
    "WJMWdFC1ai2W"
   ],
   "name": "Copy of LSTM_Donors_Assignment.ipynb",
   "provenance": [
    {
     "file_id": "1ywjbPeq8fsGJTXQCaPJZClRx38-UjK8o",
     "timestamp": 1565350368775
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
