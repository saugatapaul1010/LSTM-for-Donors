{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "global start \n",
    "start = dt.now()\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the data and displaying the initial tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the given dataframes.\n",
    "project_data = pd.read_csv('train_data.csv')\n",
    "resource_data = pd.read_csv('resources.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_submitted_datetime</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160221</td>\n",
       "      <td>p253737</td>\n",
       "      <td>c90749f5d961ff158d4b4d1e7dc665fc</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>IN</td>\n",
       "      <td>2016-12-05 13:43:57</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>ESL, Literacy</td>\n",
       "      <td>Educational Support for English Learners at Home</td>\n",
       "      <td>My students are English learners that are work...</td>\n",
       "      <td>\\\"The limits of your language are the limits o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need opportunities to practice beg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140945</td>\n",
       "      <td>p258326</td>\n",
       "      <td>897464ce9ddc600bced1151f324dd63a</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>FL</td>\n",
       "      <td>2016-10-25 09:22:10</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>History &amp; Civics, Health &amp; Sports</td>\n",
       "      <td>Civics &amp; Government, Team Sports</td>\n",
       "      <td>Wanted: Projector for Hungry Learners</td>\n",
       "      <td>Our students arrive to our school eager to lea...</td>\n",
       "      <td>The projector we need for our school is very c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need a projector to help with view...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                        teacher_id teacher_prefix  \\\n",
       "0      160221  p253737  c90749f5d961ff158d4b4d1e7dc665fc           Mrs.   \n",
       "1      140945  p258326  897464ce9ddc600bced1151f324dd63a            Mr.   \n",
       "\n",
       "  school_state project_submitted_datetime project_grade_category  \\\n",
       "0           IN        2016-12-05 13:43:57          Grades PreK-2   \n",
       "1           FL        2016-10-25 09:22:10             Grades 6-8   \n",
       "\n",
       "          project_subject_categories     project_subject_subcategories  \\\n",
       "0                Literacy & Language                     ESL, Literacy   \n",
       "1  History & Civics, Health & Sports  Civics & Government, Team Sports   \n",
       "\n",
       "                                      project_title  \\\n",
       "0  Educational Support for English Learners at Home   \n",
       "1             Wanted: Projector for Hungry Learners   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  My students are English learners that are work...   \n",
       "1  Our students arrive to our school eager to lea...   \n",
       "\n",
       "                                     project_essay_2 project_essay_3  \\\n",
       "0  \\\"The limits of your language are the limits o...             NaN   \n",
       "1  The projector we need for our school is very c...             NaN   \n",
       "\n",
       "  project_essay_4                           project_resource_summary  \\\n",
       "0             NaN  My students need opportunities to practice beg...   \n",
       "1             NaN  My students need a projector to help with view...   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \n",
       "0                                             0                    0  \n",
       "1                                             7                    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the contents of \"train_data.csv\"\n",
    "project_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p233245</td>\n",
       "      <td>LC652 - Lakeshore Double-Space Mobile Drying Rack</td>\n",
       "      <td>1</td>\n",
       "      <td>149.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p069063</td>\n",
       "      <td>Bouncy Bands for Desks (Blue support pipes)</td>\n",
       "      <td>3</td>\n",
       "      <td>14.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                        description  quantity  \\\n",
       "0  p233245  LC652 - Lakeshore Double-Space Mobile Drying Rack         1   \n",
       "1  p069063        Bouncy Bands for Desks (Blue support pipes)         3   \n",
       "\n",
       "    price  \n",
       "0  149.00  \n",
       "1   14.95  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the contents of \"resources.csv\"\n",
    "resource_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merge the two dataframes based on ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_submitted_datetime</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mrs.</td>\n",
       "      <td>IN</td>\n",
       "      <td>2016-12-05 13:43:57</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>ESL, Literacy</td>\n",
       "      <td>Educational Support for English Learners at Home</td>\n",
       "      <td>My students are English learners that are work...</td>\n",
       "      <td>\\\"The limits of your language are the limits o...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>My students need opportunities to practice beg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>154.60</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr.</td>\n",
       "      <td>FL</td>\n",
       "      <td>2016-10-25 09:22:10</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>History &amp; Civics, Health &amp; Sports</td>\n",
       "      <td>Civics &amp; Government, Team Sports</td>\n",
       "      <td>Wanted: Projector for Hungry Learners</td>\n",
       "      <td>Our students arrive to our school eager to lea...</td>\n",
       "      <td>The projector we need for our school is very c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>My students need a projector to help with view...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>299.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms.</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2016-08-31 12:03:56</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>Health &amp; Sports</td>\n",
       "      <td>Health &amp; Wellness, Team Sports</td>\n",
       "      <td>Soccer Equipment for AWESOME Middle School Stu...</td>\n",
       "      <td>\\r\\n\\\"True champions aren't always the ones th...</td>\n",
       "      <td>The students on the campus come to school know...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>My students need shine guards, athletic socks,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>516.85</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  teacher_prefix school_state project_submitted_datetime  \\\n",
       "0           Mrs.           IN        2016-12-05 13:43:57   \n",
       "1            Mr.           FL        2016-10-25 09:22:10   \n",
       "2            Ms.           AZ        2016-08-31 12:03:56   \n",
       "\n",
       "  project_grade_category         project_subject_categories  \\\n",
       "0          Grades PreK-2                Literacy & Language   \n",
       "1             Grades 6-8  History & Civics, Health & Sports   \n",
       "2             Grades 6-8                    Health & Sports   \n",
       "\n",
       "      project_subject_subcategories  \\\n",
       "0                     ESL, Literacy   \n",
       "1  Civics & Government, Team Sports   \n",
       "2    Health & Wellness, Team Sports   \n",
       "\n",
       "                                       project_title  \\\n",
       "0   Educational Support for English Learners at Home   \n",
       "1              Wanted: Projector for Hungry Learners   \n",
       "2  Soccer Equipment for AWESOME Middle School Stu...   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  My students are English learners that are work...   \n",
       "1  Our students arrive to our school eager to lea...   \n",
       "2  \\r\\n\\\"True champions aren't always the ones th...   \n",
       "\n",
       "                                     project_essay_2 project_essay_3  \\\n",
       "0  \\\"The limits of your language are the limits o...                   \n",
       "1  The projector we need for our school is very c...                   \n",
       "2  The students on the campus come to school know...                   \n",
       "\n",
       "  project_essay_4                           project_resource_summary  \\\n",
       "0                  My students need opportunities to practice beg...   \n",
       "1                  My students need a projector to help with view...   \n",
       "2                  My students need shine guards, athletic socks,...   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved   price  \\\n",
       "0                                             0                    0  154.60   \n",
       "1                                             7                    1  299.00   \n",
       "2                                             1                    0  516.85   \n",
       "\n",
       "   quantity  \n",
       "0        23  \n",
       "1         1  \n",
       "2        22  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merging both the dataframes by their corresponding IDs\n",
    "price_quantity_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\n",
    "project_data = pd.merge(project_data, price_quantity_data, on='id', how='left')\n",
    "\n",
    "#Remove the columns which are not needed anymore. Keeping ID for now.\n",
    "project_data.drop(['Unnamed: 0', 'teacher_id', 'id'], axis=1, inplace=True)\n",
    "\n",
    "project_data['teacher_prefix'] = project_data['teacher_prefix'].fillna('Mrs.')\n",
    "project_data['school_state'] = project_data['school_state'].fillna(' ')\n",
    "project_data['project_grade_category'] = project_data['project_grade_category'].fillna(' ')\n",
    "project_data['project_subject_categories'] = project_data['project_subject_categories'].fillna(' ')\n",
    "project_data['project_subject_subcategories'] = project_data['project_subject_subcategories'].fillna(' ')\n",
    "project_data['project_title'] = project_data['project_title'].fillna(' ')\n",
    "project_data['project_essay_1'] = project_data['project_essay_1'].fillna(' ')\n",
    "project_data['project_essay_2'] = project_data['project_essay_2'].fillna(' ')\n",
    "project_data['project_essay_3'] = project_data['project_essay_3'].fillna(' ')\n",
    "project_data['project_essay_4'] = project_data['project_essay_4'].fillna(' ')\n",
    "project_data['project_resource_summary'] = project_data['project_resource_summary'].fillna(' ')\n",
    "project_data['teacher_number_of_previously_posted_projects'] = project_data['teacher_number_of_previously_posted_projects'].fillna(0)\n",
    "project_data['price'] = project_data['price'].fillna(0)\n",
    "project_data['quantity'] = project_data['quantity'].fillna(0)\n",
    "\n",
    "project_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points we have:  109248\n",
      "Number of initial features we have: 16\n",
      "\n",
      "Let's look at the all columns present in the dataset: \n",
      " ['teacher_prefix' 'school_state' 'project_submitted_datetime'\n",
      " 'project_grade_category' 'project_subject_categories'\n",
      " 'project_subject_subcategories' 'project_title' 'project_essay_1'\n",
      " 'project_essay_2' 'project_essay_3' 'project_essay_4'\n",
      " 'project_resource_summary' 'teacher_number_of_previously_posted_projects'\n",
      " 'project_is_approved' 'price' 'quantity']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data points we have: \", project_data.shape[0])\n",
    "print(\"Number of initial features we have:\", project_data.shape[1]) \n",
    "print(\"\\nLet's look at the all columns present in the dataset: \\n\",project_data.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Pre-processing Section\n",
    "\n",
    "In this section, we will pre-process all the data before using them to build Machine Learning models. The dataset has the following types of features:\n",
    "\n",
    "<b>Categorical variables:</b>\n",
    "\n",
    "1. teacher_prefix\n",
    "2. school_state\n",
    "3. project_grade_category\n",
    "4. project_subject_categories\n",
    "5. project_subject_subcategories\n",
    "\n",
    "<b>Text data:</b>\n",
    "\n",
    "1. project_essay_1\n",
    "2. project_essay_2\n",
    "3. project_essay_3\n",
    "4. project_essay_4\n",
    "5. project_title\n",
    "6. project_resource_summary\n",
    "\n",
    "<b>Numerical Data:</b>\n",
    "\n",
    "1. teacher_number_of_previously_posted_projects\n",
    "2. price\n",
    "3. quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Utility functions for pre processing text datas and categories data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords contains the list of commonly found english keywords. We will remove them from the text datas as part of the pre-processing of data.\n",
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\", \\\n",
    "            't','u','v','w','x','y','z']\n",
    "\n",
    "from tqdm import tqdm\n",
    "def clean_subjects(input_values):\n",
    "    '''This function will be used to pre process the two features -> \"project_subject_categories\" and \n",
    "    \"project_subject_subcategories\"'''\n",
    "    processed_list = []\n",
    "    for i in tqdm(input_values):\n",
    "        temp = \"\"\n",
    "        for j in i.split(','):\n",
    "            if 'The' in j.split(): \n",
    "                j=j.replace('The','') \n",
    "            j = j.replace(' ','') \n",
    "            temp +=j.strip()+\" \"\n",
    "            temp = temp.replace('&','_')\n",
    "        processed_list.append(temp.strip())\n",
    "    return processed_list\n",
    "\n",
    "import re\n",
    "def decontracted(phrase):\n",
    "    \"\"\"This function will be used to expand the de-contracted words\"\"\"\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def process_text(list_of_sentences):\n",
    "    \"\"\"This function will be used to pre-process the text data\"\"\"\n",
    "    preprocessed_texts = []\n",
    "    for sentence in tqdm(list_of_sentences):\n",
    "        sent = decontracted(sentence)\n",
    "        sent = sent.replace('\\\\r', ' ')\n",
    "        sent = sent.replace('\\\\\"', ' ')\n",
    "        sent = sent.replace('\\\\n', ' ')\n",
    "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "        sent = ' '.join(word.lower() for word in sent.split() if word.lower() not in stopwords) #We will keep only those words in title which has a string length greater than one\n",
    "        preprocessed_texts.append(sent.lower().strip())\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Pre-Processing the 'essays' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109248/109248 [02:02<00:00, 891.64it/s]\n"
     ]
    }
   ],
   "source": [
    "#Merging the essays data into one single column for ease of processing.\n",
    "project_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) + \\\n",
    "                        project_data[\"project_essay_2\"].map(str) + \\\n",
    "                        project_data[\"project_essay_3\"].map(str) + \\\n",
    "                        project_data[\"project_essay_4\"].map(str)\n",
    "\n",
    "preprocessed_essays = process_text(project_data[\"essay\"].values)\n",
    "\n",
    "#Add the pre-processed data into a new column.\n",
    "project_data['clean_essays'] = preprocessed_essays\n",
    "\n",
    "#Remove the columns which are already processed and are not needed anymore.\n",
    "project_data.drop(['essay','project_essay_1','project_essay_2','project_essay_3','project_essay_4'], \n",
    "                  axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Pre-Processing the 'project_resource_summary' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109248/109248 [00:13<00:00, 8298.11it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_summary = process_text(project_data['project_resource_summary'].values)\n",
    "project_data['clean_project_resource_summary'] = preprocessed_summary\n",
    "project_data.drop(['project_resource_summary'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Pre-Processing the 'project_title' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109248/109248 [00:05<00:00, 18776.88it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_titles = process_text(project_data['project_title'].values)\n",
    "project_data['clean_project_title'] = preprocessed_titles\n",
    "project_data.drop(['project_title'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Pre-Processing of 'project_subject_categories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109248/109248 [00:00<00:00, 300560.97it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_categories = clean_subjects(project_data['project_subject_categories'].values)\n",
    "\n",
    "project_data['clean_categories'] = preprocessed_categories\n",
    "project_data.drop(['project_subject_categories'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Pre-Processing of 'project_subject_subcategories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109248/109248 [00:00<00:00, 277047.96it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_subcategories = clean_subjects(project_data['project_subject_subcategories'].values)\n",
    "\n",
    "project_data['clean_subcategories'] = preprocessed_subcategories\n",
    "project_data.drop(['project_subject_subcategories'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Pre-Processing of 'project_grade_category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data['project_grade_category'] = project_data['project_grade_category'].map(lambda x: x.replace(\" \",\"_\").replace(\"-\",\"_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Designing a new feature called - Presence of numerical digits in project resources summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_digit(sent):\n",
    "    digits=re.findall('\\d+', sent)\n",
    "    if(len(digits) != 0 ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "project_data['presence_of_the_numerical_digits']=project_data['clean_project_resource_summary'].apply(is_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Combining all text features into one single feature 'total_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data['total_text'] = project_data['clean_essays'].map(str) + \" \" + project_data['clean_project_resource_summary'].map(str) + \" \" + project_data['clean_project_title'].map(str)\n",
    "project_data.drop(['clean_essays','clean_project_resource_summary','clean_project_title','project_submitted_datetime'], axis=1, inplace=True) #Remove not needed features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9 Replacing NAN values by empty strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data['teacher_prefix'] = project_data['teacher_prefix'].fillna('Mrs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.10 Display the processed dataframe and save it into a pandas CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>presence_of_the_numerical_digits</th>\n",
       "      <th>total_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mrs.</td>\n",
       "      <td>IN</td>\n",
       "      <td>Grades_PreK_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>154.60</td>\n",
       "      <td>23</td>\n",
       "      <td>Literacy_Language</td>\n",
       "      <td>ESL Literacy</td>\n",
       "      <td>0</td>\n",
       "      <td>students english learners working english seco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr.</td>\n",
       "      <td>FL</td>\n",
       "      <td>Grades_6_8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>299.00</td>\n",
       "      <td>1</td>\n",
       "      <td>History_Civics Health_Sports</td>\n",
       "      <td>Civics_Government TeamSports</td>\n",
       "      <td>0</td>\n",
       "      <td>students arrive school eager learn polite gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms.</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Grades_6_8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>516.85</td>\n",
       "      <td>22</td>\n",
       "      <td>Health_Sports</td>\n",
       "      <td>Health_Wellness TeamSports</td>\n",
       "      <td>0</td>\n",
       "      <td>true champions not always ones win guts mia ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mrs.</td>\n",
       "      <td>KY</td>\n",
       "      <td>Grades_PreK_2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>232.90</td>\n",
       "      <td>4</td>\n",
       "      <td>Literacy_Language Math_Science</td>\n",
       "      <td>Literacy Mathematics</td>\n",
       "      <td>0</td>\n",
       "      <td>work unique school filled esl english second l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mrs.</td>\n",
       "      <td>TX</td>\n",
       "      <td>Grades_PreK_2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67.98</td>\n",
       "      <td>4</td>\n",
       "      <td>Math_Science</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>0</td>\n",
       "      <td>second grade classroom next year made around 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  teacher_prefix school_state project_grade_category  \\\n",
       "0           Mrs.           IN          Grades_PreK_2   \n",
       "1            Mr.           FL             Grades_6_8   \n",
       "2            Ms.           AZ             Grades_6_8   \n",
       "3           Mrs.           KY          Grades_PreK_2   \n",
       "4           Mrs.           TX          Grades_PreK_2   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved   price  \\\n",
       "0                                             0                    0  154.60   \n",
       "1                                             7                    1  299.00   \n",
       "2                                             1                    0  516.85   \n",
       "3                                             4                    1  232.90   \n",
       "4                                             1                    1   67.98   \n",
       "\n",
       "   quantity                clean_categories           clean_subcategories  \\\n",
       "0        23               Literacy_Language                  ESL Literacy   \n",
       "1         1    History_Civics Health_Sports  Civics_Government TeamSports   \n",
       "2        22                   Health_Sports    Health_Wellness TeamSports   \n",
       "3         4  Literacy_Language Math_Science          Literacy Mathematics   \n",
       "4         4                    Math_Science                   Mathematics   \n",
       "\n",
       "   presence_of_the_numerical_digits  \\\n",
       "0                                 0   \n",
       "1                                 0   \n",
       "2                                 0   \n",
       "3                                 0   \n",
       "4                                 0   \n",
       "\n",
       "                                          total_text  \n",
       "0  students english learners working english seco...  \n",
       "1  students arrive school eager learn polite gene...  \n",
       "2  true champions not always ones win guts mia ha...  \n",
       "3  work unique school filled esl english second l...  \n",
       "4  second grade classroom next year made around 2...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After pre-processing, we have 13 features. We will have one feature as our target variable and the rest of the columns as our independent features variables.\n",
    "project_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the processed dataset into a pandas CSV file.\n",
    "project_data.to_csv(\"processed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data = pd.read_csv(\"processed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Splitting the original data into train and test data in 80:20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in train data:  87398\n",
      "Number of points in validation data:  21850\n"
     ]
    }
   ],
   "source": [
    "#Taking the target and predictor variables into separate variables\n",
    "y = project_data[\"project_is_approved\"] #target variables\n",
    "X = project_data.drop(['project_is_approved'], axis=1) #predictor variables\n",
    "\n",
    "#Split the dataset into train and test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1, stratify=y)\n",
    "\n",
    "#Display basic information after splitting the data\n",
    "print(\"Number of points in train data: \",X_train.shape[0])\n",
    "print(\"Number of points in validation data: \",X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the total_text values in list\n",
    "docs_text_train=list(X_train.total_text.values)\n",
    "docs_text_test=list(X_test.total_text.values)\n",
    "labels_train=np.array(y_train)\n",
    "labels_test=np.array(y_test)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_text_train)\n",
    "\n",
    "#Convert the texts to sequences using the tokenizer\n",
    "sequences_text_train = tokens.texts_to_sequences(docs_text_train)\n",
    "sequences_text_test = tokens.texts_to_sequences(docs_text_test)\n",
    "vocab_size_text = len(tokens.word_index) + 1\n",
    "\n",
    "#Add padding\n",
    "padded_text_train = pad_sequences(sequences_text_train, maxlen=300, padding='post')\n",
    "padded_text_test = pad_sequences(sequences_text_test, maxlen=300, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55638/55638 [00:00<00:00, 300711.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "file = open('glove.6B.300d.txt')\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "file.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#Create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size_text, 300))\n",
    "for word, i in tqdm(tokens.word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector #embedding_matrix.shape: (9049, 300)\n",
    "\n",
    "print(len(embedding_matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Get the flattened LSTM output for input text\n",
    "input_layer_total_text = Input(shape=(300,), name = \"total_text_sequence\")\n",
    "embedding_layer_total_text = Embedding(input_dim=vocab_size_text, output_dim=300, weights=[embedding_matrix], trainable=False)(input_layer_total_text)\n",
    "lstm_total_text  = LSTM(32, activation=\"relu\", return_sequences=True)(embedding_layer_total_text)\n",
    "flatten_total_text = Flatten()(lstm_total_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data: school_state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the school state values\n",
    "docs_school_state_train=list(X_train.school_state.values)\n",
    "docs_school_state_test=list(X_test.school_state.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_school_state_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_school_train = np.array(tokens.texts_to_sequences(docs_school_state_train))\n",
    "sequences_school_test = np.array(tokens.texts_to_sequences(docs_school_state_test))\n",
    "vocab_size_school_state = len(tokens.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the flattened output for school_state\n",
    "input_layer_school_state = Input(shape=(1,), name = \"encoded_school_state\")\n",
    "embedding_layer_school_state = Embedding(input_dim=vocab_size_school_state, output_dim=4, trainable=True)(input_layer_school_state)\n",
    "flatten_school_state = Flatten()(embedding_layer_school_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data: teacher_prefix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the teacher_prefix values\n",
    "docs_teacher_prefix_train=list(X_train.teacher_prefix.values)\n",
    "docs_teacher_prefix_test=list(X_test.teacher_prefix.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_teacher_prefix_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_teacher_prefix_train = np.array(tokens.texts_to_sequences(docs_teacher_prefix_train))\n",
    "sequences_teacher_prefix_test = np.array(tokens.texts_to_sequences(docs_teacher_prefix_test))\n",
    "vocab_size_teacher_prefix = len(tokens.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_prefix\n",
    "input_layer_teacher_prefix = Input(shape=(1,), name = \"teacher_prefix\")\n",
    "embedding_layer_teacher_prefix = Embedding(input_dim=vocab_size_teacher_prefix, output_dim=4, trainable=True)(input_layer_teacher_prefix)\n",
    "flatten_teacher_prefix = Flatten()(embedding_layer_teacher_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data: project_grade_category "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the project_grade_category values\n",
    "docs_project_grade_category_train=list(X_train.project_grade_category.values)\n",
    "docs_project_grade_category_test=list(X_test.project_grade_category.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_project_grade_category_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_project_grade_category_train = tokens.texts_to_sequences(docs_project_grade_category_train)\n",
    "sequences_project_grade_category_test = tokens.texts_to_sequences(docs_project_grade_category_test)\n",
    "vocab_size_project_grade_category= len(tokens.word_index) + 1\n",
    "\n",
    "#Add padding\n",
    "padded_project_grade_category_train = pad_sequences(sequences_project_grade_category_train, maxlen=3, padding='post')\n",
    "padded_project_grade_category_test = pad_sequences(sequences_project_grade_category_test, maxlen=3, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_grade_category\n",
    "input_layer_project_grade = Input(shape=(3,), name = \"project_grade_category\")\n",
    "embedding_layer_project_grade = Embedding(input_dim=vocab_size_project_grade_category, output_dim=4, trainable=True)(input_layer_project_grade)\n",
    "flatten_project_grade = Flatten()(embedding_layer_project_grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data: clean_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the clean_categories values\n",
    "docs_clean_categories_train=list(X_train.clean_categories.values)\n",
    "docs_clean_categories_test=list(X_test.clean_categories.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_clean_categories_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_clean_categories_train = tokens.texts_to_sequences(docs_clean_categories_train)\n",
    "sequences_clean_categories_test = tokens.texts_to_sequences(docs_clean_categories_test)\n",
    "vocab_size_clean_categories = len(tokens.word_index) + 1\n",
    "\n",
    "#Add padding\n",
    "padded_clean_categories_train = pad_sequences(sequences_clean_categories_train, maxlen=3, padding='post')\n",
    "padded_clean_categories_test = pad_sequences(sequences_clean_categories_test, maxlen=3, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_categories\n",
    "input_layer_clean_categories = Input(shape=(3,), name = \"clean_categories\")\n",
    "embedding_layer_clean_categories = Embedding(input_dim=vocab_size_clean_categories, output_dim=4, trainable=True)(input_layer_clean_categories)\n",
    "flatten_clean_categories = Flatten()(embedding_layer_clean_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data: clean_subcategories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the clean_subcategories values\n",
    "docs_clean_subcategories_train=list(X_train.clean_subcategories.values)\n",
    "docs_clean_subcategories_test=list(X_test.clean_subcategories.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_clean_subcategories_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_clean_subcategories_train = tokens.texts_to_sequences(docs_clean_subcategories_train)\n",
    "sequences_clean_subcategories_test = tokens.texts_to_sequences(docs_clean_subcategories_test)\n",
    "vocab_size_clean_subcategories = len(tokens.word_index) + 1\n",
    "\n",
    "padded_clean_subcategories_train = pad_sequences(sequences_clean_subcategories_train, maxlen=3, padding='post')\n",
    "padded_clean_subcategories_test = pad_sequences(sequences_clean_subcategories_test, maxlen=3, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_subcategories\n",
    "input_layer_clean_subcategories = Input(shape=(3,), name = \"clean_subcategories\")\n",
    "embedding_layer_clean_subcategories = Embedding(input_dim=vocab_size_clean_subcategories, output_dim=4, trainable=True)(input_layer_clean_subcategories)\n",
    "flatten_clean_subcategories = Flatten()(embedding_layer_clean_subcategories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def normalize_vars(data):\n",
    "    \"\"\"This function is used to normalize all the input datas between 0 and 1\"\"\"\n",
    "    normalizer = Normalizer()\n",
    "    data_normalized = normalizer.fit_transform(data.reshape(1, -1))\n",
    "    return data_normalized, normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## teacher_number_of_previously_posted_projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_projects_train = X_train.teacher_number_of_previously_posted_projects.values\n",
    "previous_projects_test = X_test.teacher_number_of_previously_posted_projects.values\n",
    "\n",
    "norm_previous_projects_train, normalizer = normalize_vars(previous_projects_train.reshape(1,-1))\n",
    "norm_previous_projects_test = normalizer.transform(previous_projects_test.reshape(1,-1))\n",
    "\n",
    "norm_previous_projects_train = norm_previous_projects_train.reshape(len(X_train),1)\n",
    "norm_previous_projects_test = norm_previous_projects_test.reshape(len(X_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_previous_projects = Input(shape=(1,), name = \"previous_projects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_train = X_train.price.values\n",
    "price_test = X_test.price.values\n",
    "\n",
    "norm_price_train, normalizer = normalize_vars(price_train.reshape(1,-1))\n",
    "norm_price_test = normalizer.transform(price_test.reshape(1,-1))\n",
    "\n",
    "norm_price_train = norm_price_train.reshape(len(X_train),1)\n",
    "norm_price_test = norm_price_test.reshape(len(X_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_price = Input(shape=(1,), name = \"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity_train = X_train.quantity.values\n",
    "quantity_test = X_test.quantity.values\n",
    "\n",
    "norm_quantity_train, normalizer = normalize_vars(quantity_train.reshape(1,-1))\n",
    "norm_quantity_test = normalizer.transform(quantity_test.reshape(1,-1))\n",
    "\n",
    "norm_quantity_train = norm_quantity_train.reshape(len(X_train),1)\n",
    "norm_quantity_test = norm_quantity_test.reshape(len(X_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_quantity = Input(shape=(1,), name = \"quantity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation of all the numerical features layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features_layers_concat = concatenate([input_layer_previous_projects, input_layer_price, input_layer_quantity])\n",
    "dense_layer_numerical = Dense(16, activation='relu',kernel_initializer='he_normal')(numerical_features_layers_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation of all the layers and building the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "total_text_sequence (InputLayer (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 300, 300)     16691700    total_text_sequence[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "teacher_prefix (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoded_school_state (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "project_grade_category (InputLa (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_categories (InputLayer)   (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_subcategories (InputLayer (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "previous_projects (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "price (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "quantity (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300, 32)      42624       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 4)         24          teacher_prefix[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 4)         208         encoded_school_state[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 3, 4)         40          project_grade_category[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 3, 4)         64          clean_categories[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 3, 4)         152         clean_subcategories[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3)            0           previous_projects[0][0]          \n",
      "                                                                 price[0][0]                      \n",
      "                                                                 quantity[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 9600)         0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 4)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4)            0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 12)           0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12)           0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 12)           0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           64          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 9660)         0           flatten_1[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_1 (Dense)           (None, 8)            77288       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8)            0           dense_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_2 (Dense)           (None, 8)            72          dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 8)            0           dense_layer_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 1)            9           dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 16,812,245\n",
      "Trainable params: 120,545\n",
      "Non-trainable params: 16,691,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Merge all the layers according to the architecture diagram\n",
    "x = concatenate([flatten_total_text, flatten_teacher_prefix, flatten_school_state, flatten_project_grade, flatten_clean_categories, flatten_clean_subcategories, dense_layer_numerical])\n",
    "x = Dense(8, activation='relu',kernel_initializer='he_normal', name='dense_layer_1')(x)\n",
    "x = Dropout(0.4, name='dropout_1')(x)\n",
    "x = Dense(8, activation='relu',kernel_initializer='he_normal',name='dense_layer_2')(x)\n",
    "x = Dropout(0.4, name='dropout_2')(x)\n",
    "output_layer = Dense(1, activation='sigmoid', name='output_layer')(x)\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=[input_layer_total_text,input_layer_teacher_prefix,input_layer_school_state,input_layer_project_grade,input_layer_clean_categories,\n",
    "                      input_layer_clean_subcategories,input_layer_previous_projects,input_layer_price,input_layer_quantity], outputs=[output_layer])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a custom metric AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as sklm\n",
    "import tensorflow as tf\n",
    "def auc_roc(y_true, y_pred):\n",
    "    return tf.py_func(sklm.roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a TensorBoard callback object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(norm_quantity_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87398 samples, validate on 21850 samples\n",
      "Epoch 1/10\n",
      "87398/87398 [==============================] - 475s 5ms/step - loss: 0.4309 - acc: 0.8481 - auc_roc: 0.6322 - val_loss: 0.3985 - val_acc: 0.8486 - val_auc_roc: 0.7211\n",
      "Epoch 2/10\n",
      "87398/87398 [==============================] - 476s 5ms/step - loss: 0.4046 - acc: 0.8486 - auc_roc: 0.6902 - val_loss: 0.4057 - val_acc: 0.8486 - val_auc_roc: 0.7186\n",
      "Epoch 3/10\n",
      "44160/87398 [==============>...............] - ETA: 3:44 - loss: 0.3956 - acc: 0.8505 - auc_roc: 0.7068"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\nTraceback (most recent call last):\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 207, in __call__\n    ret = func(*args)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\", line 356, in roc_auc_score\n    sample_weight=sample_weight)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/base.py\", line 77, in _average_binary_score\n    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\", line 328, in _binary_roc_auc_score\n    sample_weight=sample_weight)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\", line 618, in roc_curve\n    y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\", line 403, in _binary_clf_curve\n    assert_all_finite(y_score)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 68, in assert_all_finite\n    _assert_all_finite(X.data if sp.issparse(X) else X, allow_nan)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 56, in _assert_all_finite\n    raise ValueError(msg_err.format(type_err, X.dtype))\n\nValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n\n\n\t [[{{node metrics_2/auc_roc/PyFunc}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-d6f6a9d9d395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m           callbacks=[tensorboard])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\nTraceback (most recent call last):\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 207, in __call__\n    ret = func(*args)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\", line 356, in roc_auc_score\n    sample_weight=sample_weight)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/base.py\", line 77, in _average_binary_score\n    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\", line 328, in _binary_roc_auc_score\n    sample_weight=sample_weight)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\", line 618, in roc_curve\n    y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\", line 403, in _binary_clf_curve\n    assert_all_finite(y_score)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 68, in assert_all_finite\n    _assert_all_finite(X.data if sp.issparse(X) else X, allow_nan)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 56, in _assert_all_finite\n    raise ValueError(msg_err.format(type_err, X.dtype))\n\nValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n\n\n\t [[{{node metrics_2/auc_roc/PyFunc}}]]"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy',auc_roc])\n",
    "model.fit(x=[padded_text_train,sequences_teacher_prefix_train,sequences_school_train,padded_project_grade_category_train,padded_clean_categories_train,padded_clean_subcategories_train,\n",
    "             norm_previous_projects_train,norm_price_train,norm_quantity_train], \n",
    "          y=[labels_train],\n",
    "          validation_data=([padded_text_test,sequences_teacher_prefix_test,sequences_school_test,padded_project_grade_category_test,padded_clean_categories_test,padded_clean_subcategories_test,\n",
    "                            norm_previous_projects_test,norm_price_test,norm_quantity_test],[labels_test]),\n",
    "          epochs=10, \n",
    "          batch_size=64, \n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.13.1 at http://saugata:6006 (Press CTRL+C to quit)\n",
      "W0803 20:47:22.782032 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:22.791897 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:22.869849 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:22.880777 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:22.977978 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.083910 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.099397 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:23.200983 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.221729 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:23.341886 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.362459 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:23.396138 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.398497 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:23.436110 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.437517 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:23.493737 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.495857 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:23.535486 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.591957 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.638803 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.643734 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:23.695354 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.701968 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:23.729485 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.730707 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:23.775298 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.778222 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:23.823245 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.825703 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:23.864517 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.866962 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:23.907119 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:23.908576 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:23.992547 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:24.028857 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:24.079454 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:24.084070 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0803 20:47:24.130093 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:24.188375 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0803 20:47:24.243630 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:24.306007 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:24.399544 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:24.478989 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:24.562854 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:24.586342 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:24.611582 140400769201920 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0803 20:47:24.613163 140400769201920 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='screenshot.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\nTraceback (most recent call last):\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 207, in __call__\n    ret = func(*args)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\", line 356, in roc_auc_score\n    sample_weight=sample_weight)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/base.py\", line 77, in _average_binary_score\n    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\", line 324, in _binary_roc_auc_score\n    raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n\nValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n\n\n\t [[{{node metrics/auc_roc/PyFunc}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-8caa044dc74d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m loss, accuracy = model.evaluate([padded_docs_essay_test, padded_docs_school_state_test, padded_docs_project_grade_category_test, padded_docs_clean_categories_test, padded_docs_clean_subcategories_test, padded_docs_teacher_prefix_test, numerical_df_test],\n\u001b[0;32m----> 2\u001b[0;31m [labels_test], verbose=0)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1111\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m                                          steps=steps)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     def predict(self, x,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\nTraceback (most recent call last):\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 207, in __call__\n    ret = func(*args)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\", line 356, in roc_auc_score\n    sample_weight=sample_weight)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/base.py\", line 77, in _average_binary_score\n    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\", line 324, in _binary_roc_auc_score\n    raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n\nValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n\n\n\t [[{{node metrics/auc_roc/PyFunc}}]]"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate([padded_docs_essay_test, padded_docs_school_state_test, padded_docs_project_grade_category_test, padded_docs_clean_categories_test, padded_docs_clean_subcategories_test, padded_docs_teacher_prefix_test, numerical_df_test],\n",
    "[labels_test], verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(x=[padded_docs_essay_test, padded_docs_school_state_test, padded_docs_project_grade_category_test, padded_docs_clean_categories_test, padded_docs_clean_subcategories_test, padded_docs_teacher_prefix_test, numerical_df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=predictions.shape[0]\n",
    "y_pred=[]\n",
    "for i in range(limit):\n",
    "    if(predictions[i][0]<0.5):\n",
    "        y_pred.append(0)\n",
    "    else:\n",
    "        y_pred.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "acc_score=metrics.accuracy_score(y_test,y_pred)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "auc_score=metrics.roc_auc_score(y_test,y_pred)\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page (1/2)\n",
      "[>                                                           ] 0%\r",
      "[======>                                                     ] 11%\r",
      "Warning: Failed to load file:///mnt/0AD801EDD801D7B9/Donors Assignment/LSTM-for-Donors/custom.css (ignore)\n",
      "[===========================>                                ] 45%\r",
      "[=============================>                              ] 49%\r",
      "[=================================>                          ] 55%\r",
      "[============================================================] 100%\r",
      "Printing pages (2/2)                                               \n",
      "[>                                                           ] \r",
      "Done                                                           \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfkit\n",
    "pdfkit.from_file('Test Pad - Model 1.html', 'Test Pad - Model 1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-5d46d32f565b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    354\u001b[0m     return _average_binary_score(\n\u001b[1;32m    355\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n\u001b[0m\u001b[1;32m    325\u001b[0m                              \"is not defined in that case.\")\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_true = np.array([0, 0, 0, 0])\n",
    "y_scores = np.array([1, 0, 0, 0])\n",
    "roc_auc_score(y_true, y_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
