{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "global start \n",
    "start = dt.now()\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the data and displaying the initial tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the given dataframes.\n",
    "project_data = pd.read_csv('train_data.csv')\n",
    "resource_data = pd.read_csv('resources.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_submitted_datetime</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160221</td>\n",
       "      <td>p253737</td>\n",
       "      <td>c90749f5d961ff158d4b4d1e7dc665fc</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>IN</td>\n",
       "      <td>2016-12-05 13:43:57</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>ESL, Literacy</td>\n",
       "      <td>Educational Support for English Learners at Home</td>\n",
       "      <td>My students are English learners that are work...</td>\n",
       "      <td>\\\"The limits of your language are the limits o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need opportunities to practice beg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140945</td>\n",
       "      <td>p258326</td>\n",
       "      <td>897464ce9ddc600bced1151f324dd63a</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>FL</td>\n",
       "      <td>2016-10-25 09:22:10</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>History &amp; Civics, Health &amp; Sports</td>\n",
       "      <td>Civics &amp; Government, Team Sports</td>\n",
       "      <td>Wanted: Projector for Hungry Learners</td>\n",
       "      <td>Our students arrive to our school eager to lea...</td>\n",
       "      <td>The projector we need for our school is very c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need a projector to help with view...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                        teacher_id teacher_prefix  \\\n",
       "0      160221  p253737  c90749f5d961ff158d4b4d1e7dc665fc           Mrs.   \n",
       "1      140945  p258326  897464ce9ddc600bced1151f324dd63a            Mr.   \n",
       "\n",
       "  school_state project_submitted_datetime project_grade_category  \\\n",
       "0           IN        2016-12-05 13:43:57          Grades PreK-2   \n",
       "1           FL        2016-10-25 09:22:10             Grades 6-8   \n",
       "\n",
       "          project_subject_categories     project_subject_subcategories  \\\n",
       "0                Literacy & Language                     ESL, Literacy   \n",
       "1  History & Civics, Health & Sports  Civics & Government, Team Sports   \n",
       "\n",
       "                                      project_title  \\\n",
       "0  Educational Support for English Learners at Home   \n",
       "1             Wanted: Projector for Hungry Learners   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  My students are English learners that are work...   \n",
       "1  Our students arrive to our school eager to lea...   \n",
       "\n",
       "                                     project_essay_2 project_essay_3  \\\n",
       "0  \\\"The limits of your language are the limits o...             NaN   \n",
       "1  The projector we need for our school is very c...             NaN   \n",
       "\n",
       "  project_essay_4                           project_resource_summary  \\\n",
       "0             NaN  My students need opportunities to practice beg...   \n",
       "1             NaN  My students need a projector to help with view...   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \n",
       "0                                             0                    0  \n",
       "1                                             7                    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the contents of \"train_data.csv\"\n",
    "project_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p233245</td>\n",
       "      <td>LC652 - Lakeshore Double-Space Mobile Drying Rack</td>\n",
       "      <td>1</td>\n",
       "      <td>149.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p069063</td>\n",
       "      <td>Bouncy Bands for Desks (Blue support pipes)</td>\n",
       "      <td>3</td>\n",
       "      <td>14.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                        description  quantity  \\\n",
       "0  p233245  LC652 - Lakeshore Double-Space Mobile Drying Rack         1   \n",
       "1  p069063        Bouncy Bands for Desks (Blue support pipes)         3   \n",
       "\n",
       "    price  \n",
       "0  149.00  \n",
       "1   14.95  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the contents of \"resources.csv\"\n",
    "resource_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merge the two dataframes based on ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_submitted_datetime</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mrs.</td>\n",
       "      <td>IN</td>\n",
       "      <td>2016-12-05 13:43:57</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>ESL, Literacy</td>\n",
       "      <td>Educational Support for English Learners at Home</td>\n",
       "      <td>My students are English learners that are work...</td>\n",
       "      <td>\\\"The limits of your language are the limits o...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>My students need opportunities to practice beg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>154.60</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr.</td>\n",
       "      <td>FL</td>\n",
       "      <td>2016-10-25 09:22:10</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>History &amp; Civics, Health &amp; Sports</td>\n",
       "      <td>Civics &amp; Government, Team Sports</td>\n",
       "      <td>Wanted: Projector for Hungry Learners</td>\n",
       "      <td>Our students arrive to our school eager to lea...</td>\n",
       "      <td>The projector we need for our school is very c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>My students need a projector to help with view...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>299.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms.</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2016-08-31 12:03:56</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>Health &amp; Sports</td>\n",
       "      <td>Health &amp; Wellness, Team Sports</td>\n",
       "      <td>Soccer Equipment for AWESOME Middle School Stu...</td>\n",
       "      <td>\\r\\n\\\"True champions aren't always the ones th...</td>\n",
       "      <td>The students on the campus come to school know...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>My students need shine guards, athletic socks,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>516.85</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  teacher_prefix school_state project_submitted_datetime  \\\n",
       "0           Mrs.           IN        2016-12-05 13:43:57   \n",
       "1            Mr.           FL        2016-10-25 09:22:10   \n",
       "2            Ms.           AZ        2016-08-31 12:03:56   \n",
       "\n",
       "  project_grade_category         project_subject_categories  \\\n",
       "0          Grades PreK-2                Literacy & Language   \n",
       "1             Grades 6-8  History & Civics, Health & Sports   \n",
       "2             Grades 6-8                    Health & Sports   \n",
       "\n",
       "      project_subject_subcategories  \\\n",
       "0                     ESL, Literacy   \n",
       "1  Civics & Government, Team Sports   \n",
       "2    Health & Wellness, Team Sports   \n",
       "\n",
       "                                       project_title  \\\n",
       "0   Educational Support for English Learners at Home   \n",
       "1              Wanted: Projector for Hungry Learners   \n",
       "2  Soccer Equipment for AWESOME Middle School Stu...   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  My students are English learners that are work...   \n",
       "1  Our students arrive to our school eager to lea...   \n",
       "2  \\r\\n\\\"True champions aren't always the ones th...   \n",
       "\n",
       "                                     project_essay_2 project_essay_3  \\\n",
       "0  \\\"The limits of your language are the limits o...                   \n",
       "1  The projector we need for our school is very c...                   \n",
       "2  The students on the campus come to school know...                   \n",
       "\n",
       "  project_essay_4                           project_resource_summary  \\\n",
       "0                  My students need opportunities to practice beg...   \n",
       "1                  My students need a projector to help with view...   \n",
       "2                  My students need shine guards, athletic socks,...   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved   price  \\\n",
       "0                                             0                    0  154.60   \n",
       "1                                             7                    1  299.00   \n",
       "2                                             1                    0  516.85   \n",
       "\n",
       "   quantity  \n",
       "0        23  \n",
       "1         1  \n",
       "2        22  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merging both the dataframes by their corresponding IDs\n",
    "price_quantity_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\n",
    "project_data = pd.merge(project_data, price_quantity_data, on='id', how='left')\n",
    "\n",
    "#Remove the columns which are not needed anymore. Keeping ID for now.\n",
    "project_data.drop(['Unnamed: 0', 'teacher_id', 'id'], axis=1, inplace=True)\n",
    "\n",
    "project_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points we have:  109248\n",
      "Number of initial features we have: 16\n",
      "\n",
      "Let's look at the all columns present in the dataset: \n",
      " ['teacher_prefix' 'school_state' 'project_submitted_datetime'\n",
      " 'project_grade_category' 'project_subject_categories'\n",
      " 'project_subject_subcategories' 'project_title' 'project_essay_1'\n",
      " 'project_essay_2' 'project_essay_3' 'project_essay_4'\n",
      " 'project_resource_summary' 'teacher_number_of_previously_posted_projects'\n",
      " 'project_is_approved' 'price' 'quantity']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data points we have: \", project_data.shape[0])\n",
    "print(\"Number of initial features we have:\", project_data.shape[1]) \n",
    "print(\"\\nLet's look at the all columns present in the dataset: \\n\",project_data.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Pre-processing Section\n",
    "\n",
    "In this section, we will pre-process all the data before using them to build Machine Learning models. The dataset has the following types of features:\n",
    "\n",
    "<b>Categorical variables:</b>\n",
    "\n",
    "1. teacher_prefix\n",
    "2. school_state\n",
    "3. project_grade_category\n",
    "4. project_subject_categories\n",
    "5. project_subject_subcategories\n",
    "\n",
    "<b>Text data:</b>\n",
    "\n",
    "1. project_essay_1\n",
    "2. project_essay_2\n",
    "3. project_essay_3\n",
    "4. project_essay_4\n",
    "5. project_title\n",
    "6. project_resource_summary\n",
    "\n",
    "<b>Numerical Data:</b>\n",
    "\n",
    "1. teacher_number_of_previously_posted_projects\n",
    "2. price\n",
    "3. quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Utility functions for pre processing text datas and categories data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords contains the list of commonly found english keywords. We will remove them from the text datas as part of the pre-processing of data.\n",
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\", \\\n",
    "            't','u','v','w','x','y','z']\n",
    "\n",
    "from tqdm import tqdm\n",
    "def clean_subjects(input_values):\n",
    "    '''This function will be used to pre process the two features -> \"project_subject_categories\" and \n",
    "    \"project_subject_subcategories\"'''\n",
    "    processed_list = []\n",
    "    for i in tqdm(input_values):\n",
    "        temp = \"\"\n",
    "        for j in i.split(','):\n",
    "            if 'The' in j.split(): \n",
    "                j=j.replace('The','') \n",
    "            j = j.replace(' ','') \n",
    "            temp +=j.strip()+\" \"\n",
    "            temp = temp.replace('&','_')\n",
    "        processed_list.append(temp.strip())\n",
    "    return processed_list\n",
    "\n",
    "import re\n",
    "def decontracted(phrase):\n",
    "    \"\"\"This function will be used to expand the de-contracted words\"\"\"\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def process_text(list_of_sentences):\n",
    "    \"\"\"This function will be used to pre-process the text data\"\"\"\n",
    "    preprocessed_texts = []\n",
    "    for sentence in tqdm(list_of_sentences):\n",
    "        sent = decontracted(sentence)\n",
    "        sent = sent.replace('\\\\r', ' ')\n",
    "        sent = sent.replace('\\\\\"', ' ')\n",
    "        sent = sent.replace('\\\\n', ' ')\n",
    "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "        sent = ' '.join(word.lower() for word in sent.split() if word.lower() not in stopwords) #We will keep only those words in title which has a string length greater than one\n",
    "        preprocessed_texts.append(sent.lower().strip())\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Pre-Processing the 'essays' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109248/109248 [02:00<00:00, 905.36it/s]\n"
     ]
    }
   ],
   "source": [
    "#Merging the essays data into one single column for ease of processing.\n",
    "project_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) + \\\n",
    "                        project_data[\"project_essay_2\"].map(str) + \\\n",
    "                        project_data[\"project_essay_3\"].map(str) + \\\n",
    "                        project_data[\"project_essay_4\"].map(str)\n",
    "\n",
    "preprocessed_essays = process_text(project_data[\"essay\"].values)\n",
    "\n",
    "#Add the pre-processed data into a new column.\n",
    "project_data['clean_essays'] = preprocessed_essays\n",
    "\n",
    "#Remove the columns which are already processed and are not needed anymore.\n",
    "project_data.drop(['essay','project_essay_1','project_essay_2','project_essay_3','project_essay_4'], \n",
    "                  axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Pre-Processing the 'project_resource_summary' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109248/109248 [00:12<00:00, 8621.78it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_summary = process_text(project_data['project_resource_summary'].values)\n",
    "project_data['clean_project_resource_summary'] = preprocessed_summary\n",
    "project_data.drop(['project_resource_summary'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Pre-Processing the 'project_title' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109248/109248 [00:05<00:00, 19912.64it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_titles = process_text(project_data['project_title'].values)\n",
    "project_data['clean_project_title'] = preprocessed_titles\n",
    "project_data.drop(['project_title'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Pre-Processing of 'project_subject_categories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109248/109248 [00:00<00:00, 302094.48it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_categories = clean_subjects(project_data['project_subject_categories'].values)\n",
    "\n",
    "project_data['clean_categories'] = preprocessed_categories\n",
    "project_data.drop(['project_subject_categories'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Pre-Processing of 'project_subject_subcategories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109248/109248 [00:00<00:00, 301790.06it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_subcategories = clean_subjects(project_data['project_subject_subcategories'].values)\n",
    "\n",
    "project_data['clean_subcategories'] = preprocessed_subcategories\n",
    "project_data.drop(['project_subject_subcategories'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Pre-Processing of 'project_grade_category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data['project_grade_category'] = project_data['project_grade_category'].map(lambda x: x.replace(\" \",\"_\").replace(\"-\",\"_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Designing a new feature called - Presence of numerical digits in project resources summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_digit(sent):\n",
    "    digits=re.findall('\\d+', sent)\n",
    "    if(len(digits) != 0 ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "project_data['presence_of_the_numerical_digits']=project_data['clean_project_resource_summary'].apply(is_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Combining all text features into one single feature 'total_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data['total_text'] = project_data['clean_essays'].map(str) + \" \" + project_data['clean_project_resource_summary'].map(str) + \" \" + project_data['clean_project_title'].map(str)\n",
    "project_data.drop(['clean_essays','clean_project_resource_summary','clean_project_title','project_submitted_datetime'], axis=1, inplace=True) #Remove not needed features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9 Replacing NAN values by empty strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data['teacher_prefix'] = project_data['teacher_prefix'].fillna(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.10 Display the processed dataframe and save it into a pandas CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>presence_of_the_numerical_digits</th>\n",
       "      <th>total_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mrs.</td>\n",
       "      <td>IN</td>\n",
       "      <td>Grades_PreK_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>154.60</td>\n",
       "      <td>23</td>\n",
       "      <td>Literacy_Language</td>\n",
       "      <td>ESL Literacy</td>\n",
       "      <td>0</td>\n",
       "      <td>students english learners working english seco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr.</td>\n",
       "      <td>FL</td>\n",
       "      <td>Grades_6_8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>299.00</td>\n",
       "      <td>1</td>\n",
       "      <td>History_Civics Health_Sports</td>\n",
       "      <td>Civics_Government TeamSports</td>\n",
       "      <td>0</td>\n",
       "      <td>students arrive school eager learn polite gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms.</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Grades_6_8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>516.85</td>\n",
       "      <td>22</td>\n",
       "      <td>Health_Sports</td>\n",
       "      <td>Health_Wellness TeamSports</td>\n",
       "      <td>0</td>\n",
       "      <td>true champions not always ones win guts mia ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mrs.</td>\n",
       "      <td>KY</td>\n",
       "      <td>Grades_PreK_2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>232.90</td>\n",
       "      <td>4</td>\n",
       "      <td>Literacy_Language Math_Science</td>\n",
       "      <td>Literacy Mathematics</td>\n",
       "      <td>0</td>\n",
       "      <td>work unique school filled esl english second l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mrs.</td>\n",
       "      <td>TX</td>\n",
       "      <td>Grades_PreK_2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67.98</td>\n",
       "      <td>4</td>\n",
       "      <td>Math_Science</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>0</td>\n",
       "      <td>second grade classroom next year made around 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  teacher_prefix school_state project_grade_category  \\\n",
       "0           Mrs.           IN          Grades_PreK_2   \n",
       "1            Mr.           FL             Grades_6_8   \n",
       "2            Ms.           AZ             Grades_6_8   \n",
       "3           Mrs.           KY          Grades_PreK_2   \n",
       "4           Mrs.           TX          Grades_PreK_2   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved   price  \\\n",
       "0                                             0                    0  154.60   \n",
       "1                                             7                    1  299.00   \n",
       "2                                             1                    0  516.85   \n",
       "3                                             4                    1  232.90   \n",
       "4                                             1                    1   67.98   \n",
       "\n",
       "   quantity                clean_categories           clean_subcategories  \\\n",
       "0        23               Literacy_Language                  ESL Literacy   \n",
       "1         1    History_Civics Health_Sports  Civics_Government TeamSports   \n",
       "2        22                   Health_Sports    Health_Wellness TeamSports   \n",
       "3         4  Literacy_Language Math_Science          Literacy Mathematics   \n",
       "4         4                    Math_Science                   Mathematics   \n",
       "\n",
       "   presence_of_the_numerical_digits  \\\n",
       "0                                 0   \n",
       "1                                 0   \n",
       "2                                 0   \n",
       "3                                 0   \n",
       "4                                 0   \n",
       "\n",
       "                                          total_text  \n",
       "0  students english learners working english seco...  \n",
       "1  students arrive school eager learn polite gene...  \n",
       "2  true champions not always ones win guts mia ha...  \n",
       "3  work unique school filled esl english second l...  \n",
       "4  second grade classroom next year made around 2...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After pre-processing, we have 13 features. We will have one feature as our target variable and the rest of the columns as our independent features variables.\n",
    "project_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the processed dataset into a pandas CSV file.\n",
    "project_data.to_csv(\"processed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data = pd.read_csv(\"processed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Splitting the original data into train and test data in 80:20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in train data:  87398\n",
      "Number of points in validation data:  21850\n"
     ]
    }
   ],
   "source": [
    "#Taking the target and predictor variables into separate variables\n",
    "y = project_data[\"project_is_approved\"] #target variables\n",
    "X = project_data.drop(['project_is_approved'], axis=1) #predictor variables\n",
    "\n",
    "#Split the dataset into train and test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1, stratify=y)\n",
    "\n",
    "#Display basic information after splitting the data\n",
    "print(\"Number of points in train data: \",X_train.shape[0])\n",
    "print(\"Number of points in validation data: \",X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the total_text values in list\n",
    "docs_text_train=list(X_train.total_text.values)\n",
    "docs_text_test=list(X_test.total_text.values)\n",
    "labels_train=np.array(y_train)\n",
    "labels_test=np.array(y_test)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_text_train)\n",
    "\n",
    "#Convert the texts to sequences using the tokenizer\n",
    "sequences_text_train = tokens.texts_to_sequences(docs_text_train)\n",
    "sequences_text_test = tokens.texts_to_sequences(docs_text_test)\n",
    "vocab_size_text = len(tokens.word_index) + 1\n",
    "\n",
    "#Add padding\n",
    "padded_text_train = pad_sequences(sequences_text_train, maxlen=300, padding='post')\n",
    "padded_text_test = pad_sequences(sequences_text_test, maxlen=300, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55638/55638 [00:00<00:00, 301570.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "file = open('glove.6B.300d.txt')\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "file.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#Create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size_text, 300))\n",
    "for word, i in tqdm(tokens.word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector \n",
    "\n",
    "print(len(embedding_matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the flattened LSTM output for input text\n",
    "input_layer_total_text = Input(shape=(300,), name = \"total_text_sequence\")\n",
    "embedding_layer_total_text = Embedding(input_dim=vocab_size_text, output_dim=300, weights=[embedding_matrix], trainable=False)(input_layer_total_text)\n",
    "lstm_total_text  = LSTM(16, activation=\"relu\", return_sequences=True)(embedding_layer_total_text)\n",
    "flatten_total_text = Flatten()(lstm_total_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data: school_state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the school state values\n",
    "docs_school_state_train=list(X_train.school_state.values)\n",
    "docs_school_state_test=list(X_test.school_state.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_school_state_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_school_train = np.array(tokens.texts_to_sequences(docs_school_state_train))\n",
    "sequences_school_test = np.array(tokens.texts_to_sequences(docs_school_state_test))\n",
    "vocab_size_school_state = len(tokens.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the flattened output for school_state\n",
    "input_layer_school_state = Input(shape=(1,), name = \"encoded_school_state\")\n",
    "embedding_layer_school_state = Embedding(input_dim=vocab_size_school_state, output_dim=4, trainable=True)(input_layer_school_state)\n",
    "flatten_school_state = Flatten()(embedding_layer_school_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data: teacher_prefix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the teacher_prefix values\n",
    "docs_teacher_prefix_train=list(X_train.teacher_prefix.values)\n",
    "docs_teacher_prefix_test=list(X_test.teacher_prefix.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_teacher_prefix_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_teacher_prefix_train = np.array(tokens.texts_to_sequences(docs_teacher_prefix_train))\n",
    "sequences_teacher_prefix_test = np.array(tokens.texts_to_sequences(docs_teacher_prefix_test))\n",
    "vocab_size_teacher_prefix = len(tokens.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the flattened output for teacher_prefix\n",
    "input_layer_teacher_prefix = Input(shape=(1,), name = \"teacher_prefix\")\n",
    "embedding_layer_teacher_prefix = Embedding(input_dim=vocab_size_teacher_prefix, output_dim=4, trainable=True)(input_layer_teacher_prefix)\n",
    "flatten_teacher_prefix = Flatten()(embedding_layer_teacher_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data: project_grade_category "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the project_grade_category values\n",
    "docs_project_grade_category_train=list(X_train.project_grade_category.values)\n",
    "docs_project_grade_category_test=list(X_test.project_grade_category.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_project_grade_category_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_project_grade_category_train = tokens.texts_to_sequences(docs_project_grade_category_train)\n",
    "sequences_project_grade_category_test = tokens.texts_to_sequences(docs_project_grade_category_test)\n",
    "vocab_size_project_grade_category= len(tokens.word_index) + 1\n",
    "\n",
    "#Add padding\n",
    "padded_project_grade_category_train = pad_sequences(sequences_project_grade_category_train, maxlen=3, padding='post')\n",
    "padded_project_grade_category_test = pad_sequences(sequences_project_grade_category_test, maxlen=3, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the flattened output for project_grade_category\n",
    "input_layer_project_grade = Input(shape=(3,), name = \"project_grade_category\")\n",
    "embedding_layer_project_grade = Embedding(input_dim=vocab_size_project_grade_category, output_dim=4, trainable=True)(input_layer_project_grade)\n",
    "flatten_project_grade = Flatten()(embedding_layer_project_grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data: clean_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the clean_categories values\n",
    "docs_clean_categories_train=list(X_train.clean_categories.values)\n",
    "docs_clean_categories_test=list(X_test.clean_categories.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_clean_categories_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_clean_categories_train = tokens.texts_to_sequences(docs_clean_categories_train)\n",
    "sequences_clean_categories_test = tokens.texts_to_sequences(docs_clean_categories_test)\n",
    "vocab_size_clean_categories = len(tokens.word_index) + 1\n",
    "\n",
    "#Add padding\n",
    "padded_clean_categories_train = pad_sequences(sequences_clean_categories_train, maxlen=3, padding='post')\n",
    "padded_clean_categories_test = pad_sequences(sequences_clean_categories_test, maxlen=3, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the flattened output for clean_categories\n",
    "input_layer_clean_categories = Input(shape=(3,), name = \"clean_categories\")\n",
    "embedding_layer_clean_categories = Embedding(input_dim=vocab_size_clean_categories, output_dim=4, trainable=True)(input_layer_clean_categories)\n",
    "flatten_clean_categories = Flatten()(embedding_layer_clean_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data: clean_subcategories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the clean_subcategories values\n",
    "docs_clean_subcategories_train=list(X_train.clean_subcategories.values)\n",
    "docs_clean_subcategories_test=list(X_test.clean_subcategories.values)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_clean_subcategories_train)\n",
    "\n",
    "#Convert the school_state to sequences using the tokenizer\n",
    "sequences_clean_subcategories_train = tokens.texts_to_sequences(docs_clean_subcategories_train)\n",
    "sequences_clean_subcategories_test = tokens.texts_to_sequences(docs_clean_subcategories_test)\n",
    "vocab_size_clean_subcategories = len(tokens.word_index) + 1\n",
    "\n",
    "padded_clean_subcategories_train = pad_sequences(sequences_clean_subcategories_train, maxlen=3, padding='post')\n",
    "padded_clean_subcategories_test = pad_sequences(sequences_clean_subcategories_test, maxlen=3, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the flattened output for clean_subcategories\n",
    "input_layer_clean_subcategories = Input(shape=(3,), name = \"clean_subcategories\")\n",
    "embedding_layer_clean_subcategories = Embedding(input_dim=vocab_size_clean_subcategories, output_dim=4, trainable=True)(input_layer_clean_subcategories)\n",
    "flatten_clean_subcategories = Flatten()(embedding_layer_clean_subcategories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def normalize_vars(data):\n",
    "    \"\"\"This function is used to normalize all the input datas between 0 and 1\"\"\"\n",
    "    normalizer = Normalizer()\n",
    "    data_normalized = normalizer.fit_transform(data.reshape(1, -1))\n",
    "    return data_normalized, normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## teacher_number_of_previously_posted_projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_projects_train = X_train.teacher_number_of_previously_posted_projects.values\n",
    "previous_projects_test = X_test.teacher_number_of_previously_posted_projects.values\n",
    "\n",
    "norm_previous_projects_train, normalizer = normalize_vars(previous_projects_train.reshape(1,-1))\n",
    "norm_previous_projects_test = normalizer.transform(previous_projects_test.reshape(1,-1))\n",
    "\n",
    "norm_previous_projects_train = norm_previous_projects_train.reshape(len(X_train),1)\n",
    "norm_previous_projects_test = norm_previous_projects_test.reshape(len(X_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layer for teacher_number_of_previously_posted_projects\n",
    "input_layer_previous_projects = Input(shape=(1,), name = \"previous_projects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_train = X_train.price.values\n",
    "price_test = X_test.price.values\n",
    "\n",
    "norm_price_train, normalizer = normalize_vars(price_train.reshape(1,-1))\n",
    "norm_price_test = normalizer.transform(price_test.reshape(1,-1))\n",
    "\n",
    "norm_price_train = norm_price_train.reshape(len(X_train),1)\n",
    "norm_price_test = norm_price_test.reshape(len(X_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layer for price\n",
    "input_layer_price = Input(shape=(1,), name = \"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity_train = X_train.quantity.values\n",
    "quantity_test = X_test.quantity.values\n",
    "\n",
    "norm_quantity_train, normalizer = normalize_vars(quantity_train.reshape(1,-1))\n",
    "norm_quantity_test = normalizer.transform(quantity_test.reshape(1,-1))\n",
    "\n",
    "norm_quantity_train = norm_quantity_train.reshape(len(X_train),1)\n",
    "norm_quantity_test = norm_quantity_test.reshape(len(X_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layer for quantity\n",
    "input_layer_quantity = Input(shape=(1,), name = \"quantity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation of all the numerical features layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features_layers_concat = concatenate([input_layer_previous_projects, input_layer_price, input_layer_quantity])\n",
    "dense_layer_numerical = Dense(4, activation='relu',kernel_initializer='he_normal')(numerical_features_layers_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation of all the layers and building the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "total_text_sequence (InputLayer (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 300, 300)     16691700    total_text_sequence[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "teacher_prefix (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoded_school_state (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "project_grade_category (InputLa (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_categories (InputLayer)   (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_subcategories (InputLayer (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "previous_projects (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "price (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "quantity (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 300, 16)      20288       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 4)         24          teacher_prefix[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 4)         208         encoded_school_state[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 3, 4)         40          project_grade_category[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 3, 4)         64          clean_categories[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 3, 4)         152         clean_subcategories[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3)            0           previous_projects[0][0]          \n",
      "                                                                 price[0][0]                      \n",
      "                                                                 quantity[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4800)         0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 4)            0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 4)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12)           0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 12)           0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 12)           0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            16          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 4848)         0           flatten_2[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_1 (Dense)           (None, 8)            38792       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8)            0           dense_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_2 (Dense)           (None, 4)            36          dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4)            0           dense_layer_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 1)            5           dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 16,751,325\n",
      "Trainable params: 59,625\n",
      "Non-trainable params: 16,691,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Merge all the layers according to the architecture diagram\n",
    "x = concatenate([flatten_total_text, flatten_teacher_prefix, flatten_school_state, flatten_project_grade, flatten_clean_categories, flatten_clean_subcategories, dense_layer_numerical])\n",
    "x = Dense(8, activation='relu',kernel_initializer='he_normal', name='dense_layer_1')(x)\n",
    "x = Dropout(0.3, name='dropout_1')(x)\n",
    "x = Dense(4, activation='relu',kernel_initializer='he_normal',name='dense_layer_2')(x)\n",
    "x = Dropout(0.3, name='dropout_2')(x)\n",
    "output_layer = Dense(1, activation='sigmoid', name='output_layer')(x)\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=[input_layer_total_text,input_layer_teacher_prefix,input_layer_school_state,input_layer_project_grade,input_layer_clean_categories,\n",
    "                      input_layer_clean_subcategories,input_layer_previous_projects,input_layer_price,input_layer_quantity], outputs=[output_layer])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a custom metric AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "#https://stackoverflow.com/questions/51922500/tf-metrics-auc-yielding-very-different-from-sklearn-metrics-roc-auc-score\n",
    "def roc_auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, EarlyStopping\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time))\n",
    "#early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto', baseline=None, restore_best_weights=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:526: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:788: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 87398 samples, validate on 21850 samples\n",
      "Epoch 1/40\n",
      "87398/87398 [==============================] - 140s 2ms/step - loss: 0.4976 - roc_auc: 0.5220 - val_loss: 0.4034 - val_roc_auc: 0.5530\n",
      "Epoch 2/40\n",
      "87398/87398 [==============================] - 136s 2ms/step - loss: 0.4625 - roc_auc: 0.5716 - val_loss: 0.4008 - val_roc_auc: 0.5858\n",
      "Epoch 3/40\n",
      "87398/87398 [==============================] - 139s 2ms/step - loss: 0.4472 - roc_auc: 0.5962 - val_loss: 0.3911 - val_roc_auc: 0.6056\n",
      "Epoch 4/40\n",
      "87398/87398 [==============================] - 139s 2ms/step - loss: 0.4367 - roc_auc: 0.6127 - val_loss: 0.3889 - val_roc_auc: 0.6186\n",
      "Epoch 5/40\n",
      "87398/87398 [==============================] - 141s 2ms/step - loss: 0.4296 - roc_auc: 0.6236 - val_loss: 0.3884 - val_roc_auc: 0.6277\n",
      "Epoch 6/40\n",
      "87398/87398 [==============================] - 140s 2ms/step - loss: 0.4243 - roc_auc: 0.6311 - val_loss: 0.3851 - val_roc_auc: 0.6343\n",
      "Epoch 7/40\n",
      "87398/87398 [==============================] - 137s 2ms/step - loss: 0.4176 - roc_auc: 0.6374 - val_loss: 0.3811 - val_roc_auc: 0.6402\n",
      "Epoch 8/40\n",
      "87398/87398 [==============================] - 140s 2ms/step - loss: 0.4134 - roc_auc: 0.6427 - val_loss: 0.3815 - val_roc_auc: 0.6452\n",
      "Epoch 9/40\n",
      "87398/87398 [==============================] - 142s 2ms/step - loss: 0.4078 - roc_auc: 0.6476 - val_loss: 0.3816 - val_roc_auc: 0.6498\n",
      "Epoch 10/40\n",
      "87398/87398 [==============================] - 141s 2ms/step - loss: 0.4034 - roc_auc: 0.6521 - val_loss: 0.3850 - val_roc_auc: 0.6541\n",
      "Epoch 11/40\n",
      "87398/87398 [==============================] - 143s 2ms/step - loss: 0.3988 - roc_auc: 0.6563 - val_loss: 0.3858 - val_roc_auc: 0.6582\n",
      "Epoch 12/40\n",
      "87398/87398 [==============================] - 142s 2ms/step - loss: 0.3961 - roc_auc: 0.6602 - val_loss: 0.3863 - val_roc_auc: 0.6620\n",
      "Epoch 13/40\n",
      "87398/87398 [==============================] - 144s 2ms/step - loss: 0.3920 - roc_auc: 0.6639 - val_loss: 0.3895 - val_roc_auc: 0.6658\n",
      "Epoch 14/40\n",
      "87398/87398 [==============================] - 141s 2ms/step - loss: 0.3875 - roc_auc: 0.6678 - val_loss: 0.3904 - val_roc_auc: 0.6696\n",
      "Epoch 15/40\n",
      "87398/87398 [==============================] - 142s 2ms/step - loss: 0.3833 - roc_auc: 0.6715 - val_loss: 0.3917 - val_roc_auc: 0.6733\n",
      "Epoch 16/40\n",
      "87398/87398 [==============================] - 137s 2ms/step - loss: 0.3800 - roc_auc: 0.6752 - val_loss: 0.3942 - val_roc_auc: 0.6770\n",
      "Epoch 17/40\n",
      "87398/87398 [==============================] - 140s 2ms/step - loss: 0.3761 - roc_auc: 0.6789 - val_loss: 0.4024 - val_roc_auc: 0.6806\n",
      "Epoch 18/40\n",
      "87398/87398 [==============================] - 141s 2ms/step - loss: 0.3735 - roc_auc: 0.6823 - val_loss: 0.4000 - val_roc_auc: 0.6840\n",
      "Epoch 19/40\n",
      "87398/87398 [==============================] - 141s 2ms/step - loss: 0.3689 - roc_auc: 0.6858 - val_loss: 0.4053 - val_roc_auc: 0.6874\n",
      "Epoch 20/40\n",
      "87398/87398 [==============================] - 141s 2ms/step - loss: 0.3669 - roc_auc: 0.6891 - val_loss: 0.4087 - val_roc_auc: 0.6907\n",
      "Epoch 21/40\n",
      "87398/87398 [==============================] - 143s 2ms/step - loss: 0.3654 - roc_auc: 0.6922 - val_loss: 0.4161 - val_roc_auc: 0.6936\n",
      "Epoch 22/40\n",
      "87398/87398 [==============================] - 140s 2ms/step - loss: 0.3639 - roc_auc: 0.6951 - val_loss: 0.4149 - val_roc_auc: 0.6964\n",
      "Epoch 23/40\n",
      "87398/87398 [==============================] - 142s 2ms/step - loss: 0.3595 - roc_auc: 0.6979 - val_loss: 0.4262 - val_roc_auc: 0.6992\n",
      "Epoch 24/40\n",
      "87398/87398 [==============================] - 138s 2ms/step - loss: 0.3571 - roc_auc: 0.7006 - val_loss: 0.4177 - val_roc_auc: 0.7019\n",
      "Epoch 25/40\n",
      "87398/87398 [==============================] - 136s 2ms/step - loss: 0.3536 - roc_auc: 0.7032 - val_loss: 0.4351 - val_roc_auc: 0.7045\n",
      "Epoch 26/40\n",
      "87398/87398 [==============================] - 140s 2ms/step - loss: 0.3507 - roc_auc: 0.7059 - val_loss: 0.4428 - val_roc_auc: 0.7071\n",
      "Epoch 27/40\n",
      "87398/87398 [==============================] - 144s 2ms/step - loss: 0.3483 - roc_auc: 0.7084 - val_loss: 0.4537 - val_roc_auc: 0.7095\n",
      "Epoch 28/40\n",
      "87398/87398 [==============================] - 141s 2ms/step - loss: 0.3464 - roc_auc: 0.7108 - val_loss: 0.4387 - val_roc_auc: 0.7119\n",
      "Epoch 29/40\n",
      "87398/87398 [==============================] - 139s 2ms/step - loss: 0.3441 - roc_auc: 0.7131 - val_loss: 0.4555 - val_roc_auc: 0.7142\n",
      "Epoch 30/40\n",
      "87398/87398 [==============================] - 142s 2ms/step - loss: 0.3414 - roc_auc: 0.7154 - val_loss: 0.4778 - val_roc_auc: 0.7165\n",
      "Epoch 31/40\n",
      "87398/87398 [==============================] - 141s 2ms/step - loss: 0.3380 - roc_auc: 0.7176 - val_loss: 0.4904 - val_roc_auc: 0.7187\n",
      "Epoch 32/40\n",
      "87398/87398 [==============================] - 143s 2ms/step - loss: 0.3357 - roc_auc: 0.7198 - val_loss: 0.4810 - val_roc_auc: 0.7208\n",
      "Epoch 33/40\n",
      "87398/87398 [==============================] - 141s 2ms/step - loss: 0.3347 - roc_auc: 0.7219 - val_loss: 0.4996 - val_roc_auc: 0.7228\n",
      "Epoch 34/40\n",
      "87398/87398 [==============================] - 135s 2ms/step - loss: 0.3323 - roc_auc: 0.7239 - val_loss: 0.4847 - val_roc_auc: 0.7248\n",
      "Epoch 35/40\n",
      "87398/87398 [==============================] - 135s 2ms/step - loss: 0.3315 - roc_auc: 0.7258 - val_loss: 0.4958 - val_roc_auc: 0.7267\n",
      "Epoch 36/40\n",
      "87398/87398 [==============================] - 135s 2ms/step - loss: 0.3289 - roc_auc: 0.7276 - val_loss: 0.5261 - val_roc_auc: 0.7285\n",
      "Epoch 37/40\n",
      "87398/87398 [==============================] - 135s 2ms/step - loss: 0.3259 - roc_auc: 0.7294 - val_loss: 0.5066 - val_roc_auc: 0.7302\n",
      "Epoch 38/40\n",
      "87398/87398 [==============================] - 135s 2ms/step - loss: 0.3234 - roc_auc: 0.7311 - val_loss: 0.5371 - val_roc_auc: 0.7319\n",
      "Epoch 39/40\n",
      "87398/87398 [==============================] - 136s 2ms/step - loss: 0.3230 - roc_auc: 0.7328 - val_loss: 0.5779 - val_roc_auc: 0.7335\n",
      "Epoch 40/40\n",
      "87398/87398 [==============================] - 140s 2ms/step - loss: 0.3186 - roc_auc: 0.7344 - val_loss: 0.5457 - val_roc_auc: 0.7352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4f1c26c048>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[roc_auc])\n",
    "model.fit(x=[padded_text_train,sequences_teacher_prefix_train,sequences_school_train,padded_project_grade_category_train,padded_clean_categories_train,padded_clean_subcategories_train,\n",
    "             norm_previous_projects_train,norm_price_train,norm_quantity_train], \n",
    "          y=[labels_train],\n",
    "          validation_data=([padded_text_test,sequences_teacher_prefix_test,sequences_school_test,padded_project_grade_category_test,padded_clean_categories_test,padded_clean_subcategories_test,\n",
    "                            norm_previous_projects_test,norm_price_test,norm_quantity_test],[labels_test]),\n",
    "          epochs=40, \n",
    "          batch_size=1024, \n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.13.1 at http://saugata:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train the TF-IDF on the Train data <br>\n",
    "2. Get the idf value for each word we have in the train data. <br>\n",
    "3. Remove the low idf value and high idf value words from our data. Do some analysis on the Idf values and based on those values choose the low and high threshold value. Because very frequent words and very very rare words don't give much information. (you can plot a box plots and take only the idf scores within IQR range and corresponding words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words present originally: 55628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Fit the TFIDF vectorizer to the train data\n",
    "vect = TfidfVectorizer()\n",
    "vect.fit_transform(X_train['total_text'])\n",
    "\n",
    "#Get the features names and their corresponding IDF scores\n",
    "words = vect.get_feature_names()\n",
    "idf_words = vect.idf_\n",
    "\n",
    "#Map the words and their idf_ scores in a disctionary\n",
    "dict_word_idf_ = dict(zip(words, idf_words))\n",
    "\n",
    "print(\"Total number of unique words present originally:\", len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-Plot Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest significant value of TF-IDF Scores:  6.4\n",
      "The highest significant value of TF-IDF Scores:  11.685091939370627\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.boxplot(idf_words)\n",
    "plt.title('Box Plot Analysis')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('IDF Score')\n",
    "plt.show()\n",
    "\n",
    "p_25th = 6.4\n",
    "p_75th = np.percentile(idf_words,75)\n",
    "\n",
    "print(\"The lowest significant value of TF-IDF Scores: \",p_25th)\n",
    "print(\"The highest significant value of TF-IDF Scores: \",p_75th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of words to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words to be removed:  2755\n"
     ]
    }
   ],
   "source": [
    "removed_wordlist = []\n",
    "for word in list(dict_word_idf_.keys()):\n",
    "    if(dict_word_idf_[word] < p_25th or dict_word_idf_[word] > p_75th):\n",
    "        removed_wordlist.append(word)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print(\"Number of words to be removed: \",len(removed_wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing words from the train and test data which falls outside the threshold range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87398/87398 [08:21<00:00, 174.13it/s]\n",
      "100%|██████████| 21850/21850 [02:05<00:00, 173.95it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_from_text(list_of_sentences):\n",
    "    \"\"\"This function will be used to remove words from text data\"\"\"\n",
    "    processed_text = []\n",
    "    for sentence in tqdm(list_of_sentences):\n",
    "        sent = ' '.join(word for word in sentence.split() if word not in removed_wordlist) #We will keep only those words in title which has a string length greater than one\n",
    "        processed_text.append(sent)\n",
    "    return processed_text\n",
    "\n",
    "X_train['total_text'] = remove_from_text(X_train.total_text.values)\n",
    "X_test['total_text'] = remove_from_text(X_test.total_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"X_train_removed.csv\", index=False)\n",
    "X_test.to_csv(\"X_test_removed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the total_text values in list\n",
    "docs_text_train=list(X_train.total_text.values)\n",
    "docs_text_test=list(X_test.total_text.values)\n",
    "labels_train=np.array(y_train)\n",
    "labels_test=np.array(y_test)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_text_train)\n",
    "\n",
    "#Convert the texts to sequences using the tokenizer\n",
    "sequences_text_train = tokens.texts_to_sequences(docs_text_train)\n",
    "sequences_text_test = tokens.texts_to_sequences(docs_text_test)\n",
    "vocab_size_text = len(tokens.word_index) + 1\n",
    "\n",
    "#Add padding\n",
    "padded_text_train = pad_sequences(sequences_text_train, maxlen=300, padding='post')\n",
    "padded_text_test = pad_sequences(sequences_text_test, maxlen=300, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52883/52883 [00:00<00:00, 294699.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "file = open('glove.6B.300d.txt')\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "file.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#Create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size_text, 300))\n",
    "for word, i in tqdm(tokens.word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector #embedding_matrix.shape: (9049, 300)\n",
    "\n",
    "print(len(embedding_matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the flattened LSTM output for input text\n",
    "input_layer_total_text = Input(shape=(300,), name = \"total_text_sequence\")\n",
    "embedding_layer_total_text = Embedding(input_dim=vocab_size_text, output_dim=300, weights=[embedding_matrix], trainable=False)(input_layer_total_text)\n",
    "lstm_total_text  = LSTM(16, activation=\"relu\", return_sequences=True)(embedding_layer_total_text)\n",
    "flatten_total_text = Flatten()(lstm_total_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the flattened output for school_state\n",
    "input_layer_school_state = Input(shape=(1,), name = \"encoded_school_state\")\n",
    "embedding_layer_school_state = Embedding(input_dim=vocab_size_school_state, output_dim=4, trainable=True)(input_layer_school_state)\n",
    "flatten_school_state = Flatten()(embedding_layer_school_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the flattened output for teacher_prefix\n",
    "input_layer_teacher_prefix = Input(shape=(1,), name = \"teacher_prefix\")\n",
    "embedding_layer_teacher_prefix = Embedding(input_dim=vocab_size_teacher_prefix, output_dim=4, trainable=True)(input_layer_teacher_prefix)\n",
    "flatten_teacher_prefix = Flatten()(embedding_layer_teacher_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the flattened output for project_grade_category\n",
    "input_layer_project_grade = Input(shape=(3,), name = \"project_grade_category\")\n",
    "embedding_layer_project_grade = Embedding(input_dim=vocab_size_project_grade_category, output_dim=4, trainable=True)(input_layer_project_grade)\n",
    "flatten_project_grade = Flatten()(embedding_layer_project_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the flattened output for clean_categories\n",
    "input_layer_clean_categories = Input(shape=(3,), name = \"clean_categories\")\n",
    "embedding_layer_clean_categories = Embedding(input_dim=vocab_size_clean_categories, output_dim=4, trainable=True)(input_layer_clean_categories)\n",
    "flatten_clean_categories = Flatten()(embedding_layer_clean_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the flattened output for clean_subcategories\n",
    "input_layer_clean_subcategories = Input(shape=(3,), name = \"clean_subcategories\")\n",
    "embedding_layer_clean_subcategories = Embedding(input_dim=vocab_size_clean_subcategories, output_dim=4, trainable=True)(input_layer_clean_subcategories)\n",
    "flatten_clean_subcategories = Flatten()(embedding_layer_clean_subcategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layer for previous_projects\n",
    "input_layer_previous_projects = Input(shape=(1,), name = \"previous_projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layer for price\n",
    "input_layer_price = Input(shape=(1,), name = \"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layer for quantity\n",
    "input_layer_quantity = Input(shape=(1,), name = \"quantity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features_layers_concat = concatenate([input_layer_previous_projects, input_layer_price, input_layer_quantity])\n",
    "dense_layer_numerical = Dense(4, activation='relu',kernel_initializer='he_normal')(numerical_features_layers_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation of all the layers and building the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "del( X_train, X_test, y_train, y_test, X, y, project_data, file, embeddings_index, coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "total_text_sequence (InputLayer (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 300, 300)     15865200    total_text_sequence[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "teacher_prefix (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoded_school_state (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "project_grade_category (InputLa (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_categories (InputLayer)   (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_subcategories (InputLayer (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "previous_projects (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "price (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "quantity (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 300, 16)      20288       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 4)         24          teacher_prefix[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 4)         208         encoded_school_state[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 3, 4)         40          project_grade_category[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 3, 4)         64          clean_categories[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 3, 4)         152         clean_subcategories[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3)            0           previous_projects[0][0]          \n",
      "                                                                 price[0][0]                      \n",
      "                                                                 quantity[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 4800)         0           lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 4)            0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 4)            0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 12)           0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 12)           0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 12)           0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            16          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 4848)         0           flatten_4[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_1 (Dense)           (None, 8)            38792       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8)            0           dense_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_2 (Dense)           (None, 4)            36          dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4)            0           dense_layer_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 1)            5           dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,924,825\n",
      "Trainable params: 59,625\n",
      "Non-trainable params: 15,865,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Merge all the layers according to the architecture diagram\n",
    "x = concatenate([flatten_total_text, flatten_teacher_prefix, flatten_school_state, flatten_project_grade, flatten_clean_categories, flatten_clean_subcategories, dense_layer_numerical])\n",
    "x = Dense(8, activation='relu',kernel_initializer='he_normal', name='dense_layer_1')(x)\n",
    "x = Dropout(0.6, name='dropout_1')(x)\n",
    "x = Dense(4, activation='relu',kernel_initializer='he_normal',name='dense_layer_2')(x)\n",
    "x = Dropout(0.6, name='dropout_2')(x)\n",
    "output_layer = Dense(1, activation='sigmoid', name='output_layer')(x)\n",
    "\n",
    "# Final model 2\n",
    "model = Model(inputs=[input_layer_total_text,input_layer_teacher_prefix,input_layer_school_state,input_layer_project_grade,input_layer_clean_categories,\n",
    "                      input_layer_clean_subcategories,input_layer_previous_projects,input_layer_price,input_layer_quantity], outputs=[output_layer])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:526: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:788: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 87398 samples, validate on 21850 samples\n",
      "Epoch 1/40\n",
      "87398/87398 [==============================] - 135s 2ms/step - loss: 0.4827 - roc_auc: 0.5160 - val_loss: 0.4260 - val_roc_auc: 0.5226\n",
      "Epoch 2/40\n",
      "87398/87398 [==============================] - 133s 2ms/step - loss: 0.4562 - roc_auc: 0.5299 - val_loss: 0.4172 - val_roc_auc: 0.5356\n",
      "Epoch 3/40\n",
      "87398/87398 [==============================] - 132s 2ms/step - loss: 0.4462 - roc_auc: 0.5408 - val_loss: 0.4175 - val_roc_auc: 0.5455\n",
      "Epoch 4/40\n",
      "87398/87398 [==============================] - 131s 1ms/step - loss: 0.4384 - roc_auc: 0.5500 - val_loss: 0.4156 - val_roc_auc: 0.5545\n",
      "Epoch 5/40\n",
      "87398/87398 [==============================] - 131s 2ms/step - loss: 0.4325 - roc_auc: 0.5585 - val_loss: 0.4155 - val_roc_auc: 0.5620\n",
      "Epoch 6/40\n",
      "87398/87398 [==============================] - 131s 1ms/step - loss: 0.4249 - roc_auc: 0.5658 - val_loss: 0.4178 - val_roc_auc: 0.5693\n",
      "Epoch 7/40\n",
      "87398/87398 [==============================] - 131s 2ms/step - loss: 0.4207 - roc_auc: 0.5729 - val_loss: 0.4158 - val_roc_auc: 0.5759\n",
      "Epoch 8/40\n",
      "87398/87398 [==============================] - 132s 2ms/step - loss: 0.4162 - roc_auc: 0.5792 - val_loss: 0.4224 - val_roc_auc: 0.5820\n",
      "Epoch 9/40\n",
      "84992/87398 [============================>.] - ETA: 3s - loss: 0.4132 - roc_auc: 0.5849"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "assertion failed: [predictions must be in [0, 1]] [Condition x <= y did not hold element-wise:x (output_layer_1/Sigmoid:0) = ] [[0.877784967][0.995404363][0.756274]...] [y (metrics/roc_auc/auc/Cast_1/x:0) = ] [1]\n\t [[{{node metrics/roc_auc/auc/assert_less_equal/Assert/AssertGuard/Assert}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-20a28ff3e12a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m           callbacks=[tensorboard])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: assertion failed: [predictions must be in [0, 1]] [Condition x <= y did not hold element-wise:x (output_layer_1/Sigmoid:0) = ] [[0.877784967][0.995404363][0.756274]...] [y (metrics/roc_auc/auc/Cast_1/x:0) = ] [1]\n\t [[{{node metrics/roc_auc/auc/assert_less_equal/Assert/AssertGuard/Assert}}]]"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[roc_auc])\n",
    "model.fit(x=[padded_text_train,sequences_teacher_prefix_train,sequences_school_train,padded_project_grade_category_train,padded_clean_categories_train,padded_clean_subcategories_train,\n",
    "             norm_previous_projects_train,norm_price_train,norm_quantity_train], \n",
    "          y=[labels_train],\n",
    "          validation_data=([padded_text_test,sequences_teacher_prefix_test,sequences_school_test,padded_project_grade_category_test,padded_clean_categories_test,padded_clean_subcategories_test,\n",
    "                            norm_previous_projects_test,norm_price_test,norm_quantity_test],[labels_test]),\n",
    "          epochs=40, \n",
    "          batch_size=1024, \n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data = pd.read_csv(\"processed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting the original data into train and test data in 80:20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in train data:  87398\n",
      "Number of points in validation data:  21850\n"
     ]
    }
   ],
   "source": [
    "#Taking the target and predictor variables into separate variables\n",
    "y = project_data[\"project_is_approved\"] #target variables\n",
    "X = project_data.drop(['project_is_approved'], axis=1) #predictor variables\n",
    "\n",
    "#Split the dataset into train and test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1, stratify=y)\n",
    "\n",
    "#Display basic information after splitting the data\n",
    "print(\"Number of points in train data: \",X_train.shape[0])\n",
    "print(\"Number of points in validation data: \",X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizing Total Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the total_text values in list\n",
    "docs_text_train=list(X_train.total_text.values)\n",
    "docs_text_test=list(X_test.total_text.values)\n",
    "labels_train=np.array(y_train)\n",
    "labels_test=np.array(y_test)\n",
    "\n",
    "#Initializing the keras tokenizer and fitting it on train data\n",
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts(docs_text_train)\n",
    "\n",
    "#Convert the texts to sequences using the tokenizer\n",
    "sequences_text_train = tokens.texts_to_sequences(docs_text_train)\n",
    "sequences_text_test = tokens.texts_to_sequences(docs_text_test)\n",
    "vocab_size_text = len(tokens.word_index) + 1\n",
    "\n",
    "#Add padding\n",
    "padded_text_train = pad_sequences(sequences_text_train, maxlen=300, padding='post')\n",
    "padded_text_test = pad_sequences(sequences_text_test, maxlen=300, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55638/55638 [00:00<00:00, 297961.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "file = open('glove.6B.300d.txt')\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "file.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#Create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size_text, 300))\n",
    "for word, i in tqdm(tokens.word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector #embedding_matrix.shape: (9049, 300)\n",
    "\n",
    "print(len(embedding_matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the flattened LSTM output for input text\n",
    "input_layer_total_text = Input(shape=(300,), name = \"total_text_sequence\")\n",
    "embedding_layer_total_text = Embedding(input_dim=vocab_size_text, output_dim=300, weights=[embedding_matrix], trainable=False)(input_layer_total_text)\n",
    "lstm_total_text  = LSTM(16, activation=\"relu\", return_sequences=True)(embedding_layer_total_text)\n",
    "flatten_lstm_out = Flatten()(lstm_total_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. One hot encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#School state\n",
    "encoder=OneHotEncoder().fit(X_train['school_state'].values.reshape(-1,1))\n",
    "enc_school_state_train=encoder.transform(X_train['school_state'].values.reshape(-1,1))\n",
    "enc_school_state_test=encoder.transform(X_test['school_state'].values.reshape(-1, 1))\n",
    "\n",
    "#Teacher prefix\n",
    "encoder=OneHotEncoder().fit(X_train['teacher_prefix'].values.reshape(-1,1))\n",
    "enc_teacher_prefix_train=encoder.transform(X_train['teacher_prefix'].values.reshape(-1,1))\n",
    "enc_teacher_prefix_test=encoder.transform(X_test['teacher_prefix'].values.reshape(-1, 1))\n",
    "\n",
    "#project_grade_category\n",
    "encoder=OneHotEncoder().fit(X_train['project_grade_category'].values.reshape(-1,1))\n",
    "enc_project_grade_category_train=encoder.transform(X_train['project_grade_category'].values.reshape(-1,1))\n",
    "enc_project_grade_category_test=encoder.transform(X_test['project_grade_category'].values.reshape(-1, 1))\n",
    "\n",
    "#clean_categories\n",
    "encoder=CountVectorizer(binary=True).fit(X_train['clean_categories'])\n",
    "enc_clean_categories_category_train=encoder.transform(X_train['clean_categories'])\n",
    "enc_clean_categories_category_test=encoder.transform(X_test['clean_categories'])\n",
    "\n",
    "#clean_subcategories\n",
    "encoder=CountVectorizer(binary=True).fit(X_train['clean_subcategories'])\n",
    "enc_clean_subcategories_train=encoder.transform(X_train['clean_subcategories'])\n",
    "enc_clean_subcategories_test=encoder.transform(X_test['clean_subcategories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normalizing numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teacher_number_of_previously_posted_projects\n",
    "previous_projects_train = X_train.teacher_number_of_previously_posted_projects.values\n",
    "previous_projects_test = X_test.teacher_number_of_previously_posted_projects.values\n",
    "\n",
    "norm_previous_projects_train, normalizer = normalize_vars(previous_projects_train.reshape(1,-1))\n",
    "norm_previous_projects_test = normalizer.transform(previous_projects_test.reshape(1,-1))\n",
    "\n",
    "norm_previous_projects_train = norm_previous_projects_train.reshape(len(X_train),1)\n",
    "norm_previous_projects_test = norm_previous_projects_test.reshape(len(X_test),1)\n",
    "\n",
    "#price\n",
    "price_train = X_train.price.values\n",
    "price_test = X_test.price.values\n",
    "\n",
    "norm_price_train, normalizer = normalize_vars(price_train.reshape(1,-1))\n",
    "norm_price_test = normalizer.transform(price_test.reshape(1,-1))\n",
    "\n",
    "norm_price_train = norm_price_train.reshape(len(X_train),1)\n",
    "norm_price_test = norm_price_test.reshape(len(X_test),1)\n",
    "\n",
    "#quantity\n",
    "quantity_train = X_train.quantity.values\n",
    "quantity_test = X_test.quantity.values\n",
    "\n",
    "norm_quantity_train, normalizer = normalize_vars(quantity_train.reshape(1,-1))\n",
    "norm_quantity_test = normalizer.transform(quantity_test.reshape(1,-1))\n",
    "\n",
    "norm_quantity_train = norm_quantity_train.reshape(len(X_train),1)\n",
    "norm_quantity_test = norm_quantity_test.reshape(len(X_test),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stacking the numerical and categorical vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "stacked_vectors_train = hstack((enc_school_state_train,enc_teacher_prefix_train,enc_project_grade_category_train,enc_clean_categories_category_train,enc_clean_subcategories_train,\n",
    "                                norm_previous_projects_train,norm_price_train,norm_quantity_train))\n",
    "\n",
    "stacked_vectors_test = hstack((enc_school_state_test,enc_teacher_prefix_test,enc_project_grade_category_test,enc_clean_categories_category_test,enc_clean_subcategories_test,\n",
    "                                norm_previous_projects_test,norm_price_test,norm_quantity_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df_train = np.expand_dims(pd.DataFrame(stacked_vectors_train.todense()), axis=2)\n",
    "encoded_df_test = np.expand_dims(pd.DataFrame(stacked_vectors_test.todense()), axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Defining the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling2D, Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "conv_input = Input(shape=(stacked_vectors_train.shape[1],1), name=\"non_text_data_layer\")\n",
    "x = Conv1D(filters=8, kernel_size=4, activation='relu', kernel_initializer='he_normal',name='conv_layer_1')(conv_input)\n",
    "x = Conv1D(filters=8, kernel_size=4, activation='relu', kernel_initializer='he_normal',name='conv_layer_2')(x)\n",
    "flatten_conv_output = Flatten()(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del( X_train, X_test, y_train, y_test, X, y, project_data, file, embeddings_index, coefs, stacked_vectors_test, stacked_vectors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "total_text_sequence (InputLayer (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "non_text_data_layer (InputLayer (None, 102, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 300, 300)     16691700    total_text_sequence[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_layer_1 (Conv1D)           (None, 99, 8)        40          non_text_data_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 300, 16)      20288       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_layer_2 (Conv1D)           (None, 96, 8)        264         conv_layer_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4800)         0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 768)          0           conv_layer_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 5568)         0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_1 (Dense)           (None, 32)           178208      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32)           0           dense_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_2 (Dense)           (None, 64)           2112        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64)           0           dense_layer_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer_3 (Dense)           (None, 128)          8320        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            129         dense_layer_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 16,901,061\n",
      "Trainable params: 209,361\n",
      "Non-trainable params: 16,691,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout\n",
    "\n",
    "x = concatenate([flatten_lstm_out, flatten_conv_output])\n",
    "\n",
    "x = Dense(32, activation='relu', kernel_initializer='he_normal',name='dense_layer_1')(x)\n",
    "x = Dropout(0.6)(x)\n",
    "x = Dense(64, activation='relu', kernel_initializer='he_normal',name='dense_layer_2')(x)\n",
    "x = Dropout(0.6)(x)\n",
    "x = Dense(128, activation='relu', kernel_initializer='he_normal',name='dense_layer_3')(x)\n",
    "\n",
    "output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "model = Model(inputs=[input_layer_total_text,conv_input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compiling the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87398 samples, validate on 21850 samples\n",
      "Epoch 1/25\n",
      "87398/87398 [==============================] - 137s 2ms/step - loss: 0.3998 - roc_auc: 0.6479 - val_loss: 0.4176 - val_roc_auc: 0.6943\n",
      "Epoch 2/25\n",
      "87398/87398 [==============================] - 136s 2ms/step - loss: 0.3746 - roc_auc: 0.7083 - val_loss: 0.4427 - val_roc_auc: 0.7151\n",
      "Epoch 3/25\n",
      "87398/87398 [==============================] - 136s 2ms/step - loss: 0.3699 - roc_auc: 0.7197 - val_loss: 0.4304 - val_roc_auc: 0.7237\n",
      "Epoch 4/25\n",
      "87398/87398 [==============================] - 137s 2ms/step - loss: 0.3639 - roc_auc: 0.7278 - val_loss: 0.4644 - val_roc_auc: 0.7296\n",
      "Epoch 5/25\n",
      "87398/87398 [==============================] - 137s 2ms/step - loss: 0.3606 - roc_auc: 0.7314 - val_loss: 0.4291 - val_roc_auc: 0.7342\n",
      "Epoch 6/25\n",
      "87398/87398 [==============================] - 137s 2ms/step - loss: 0.3569 - roc_auc: 0.7368 - val_loss: 0.5194 - val_roc_auc: 0.7364\n",
      "Epoch 7/25\n",
      "87398/87398 [==============================] - 137s 2ms/step - loss: 0.3529 - roc_auc: 0.7368 - val_loss: 0.4418 - val_roc_auc: 0.7390\n",
      "Epoch 8/25\n",
      "87398/87398 [==============================] - 138s 2ms/step - loss: 0.3494 - roc_auc: 0.7412 - val_loss: 0.4597 - val_roc_auc: 0.7429\n",
      "Epoch 9/25\n",
      "87398/87398 [==============================] - 138s 2ms/step - loss: 0.3469 - roc_auc: 0.7448 - val_loss: 0.4752 - val_roc_auc: 0.7458\n",
      "Epoch 10/25\n",
      "87398/87398 [==============================] - 136s 2ms/step - loss: 0.3443 - roc_auc: 0.7471 - val_loss: 0.4550 - val_roc_auc: 0.7486\n",
      "Epoch 11/25\n",
      "87398/87398 [==============================] - 137s 2ms/step - loss: 0.3405 - roc_auc: 0.7502 - val_loss: 0.4934 - val_roc_auc: 0.7510\n",
      "Epoch 12/25\n",
      "87398/87398 [==============================] - 138s 2ms/step - loss: 0.3397 - roc_auc: 0.7520 - val_loss: 0.4563 - val_roc_auc: 0.7532\n",
      "Epoch 13/25\n",
      "87398/87398 [==============================] - 137s 2ms/step - loss: 0.3348 - roc_auc: 0.7548 - val_loss: 0.4855 - val_roc_auc: 0.7556\n",
      "Epoch 14/25\n",
      "87398/87398 [==============================] - 137s 2ms/step - loss: 0.3326 - roc_auc: 0.7568 - val_loss: 0.4894 - val_roc_auc: 0.7576\n",
      "Epoch 15/25\n",
      "87398/87398 [==============================] - 137s 2ms/step - loss: 0.3293 - roc_auc: 0.7588 - val_loss: 0.4801 - val_roc_auc: 0.7598\n",
      "Epoch 16/25\n",
      "87398/87398 [==============================] - 137s 2ms/step - loss: 0.3273 - roc_auc: 0.7610 - val_loss: 0.4475 - val_roc_auc: 0.7622\n",
      "Epoch 17/25\n",
      "87398/87398 [==============================] - 137s 2ms/step - loss: 0.3237 - roc_auc: 0.7636 - val_loss: 0.5008 - val_roc_auc: 0.7644\n",
      "Epoch 18/25\n",
      "87398/87398 [==============================] - 138s 2ms/step - loss: 0.3191 - roc_auc: 0.7655 - val_loss: 0.4339 - val_roc_auc: 0.7667\n",
      "Epoch 19/25\n",
      "87398/87398 [==============================] - 140s 2ms/step - loss: 0.3178 - roc_auc: 0.7681 - val_loss: 0.4224 - val_roc_auc: 0.7694\n",
      "Epoch 20/25\n",
      "87398/87398 [==============================] - 140s 2ms/step - loss: 0.3155 - roc_auc: 0.7707 - val_loss: 0.4372 - val_roc_auc: 0.7719\n",
      "Epoch 21/25\n",
      "87398/87398 [==============================] - 139s 2ms/step - loss: 0.3121 - roc_auc: 0.7732 - val_loss: 0.4107 - val_roc_auc: 0.7744\n",
      "Epoch 22/25\n",
      "87398/87398 [==============================] - 140s 2ms/step - loss: 0.3094 - roc_auc: 0.7759 - val_loss: 0.4410 - val_roc_auc: 0.7768\n",
      "Epoch 23/25\n",
      "87398/87398 [==============================] - 139s 2ms/step - loss: 0.3062 - roc_auc: 0.7780 - val_loss: 0.4205 - val_roc_auc: 0.7791\n",
      "Epoch 24/25\n",
      "87398/87398 [==============================] - 139s 2ms/step - loss: 0.3045 - roc_auc: 0.7804 - val_loss: 0.4603 - val_roc_auc: 0.7813\n",
      "Epoch 25/25\n",
      "87398/87398 [==============================] - 140s 2ms/step - loss: 0.3036 - roc_auc: 0.7824 - val_loss: 0.4037 - val_roc_auc: 0.7834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbad80edcf8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[roc_auc])\n",
    "model.fit(x=[padded_text_train, encoded_df_train], \n",
    "          y=[labels_train],\n",
    "          epochs=25, \n",
    "          batch_size=1024,\n",
    "          validation_data=([padded_text_test, encoded_df_test],[labels_test]),\n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.13.1 at http://saugata:6006 (Press CTRL+C to quit)\n",
      "W0805 20:19:40.818093 139881525212928 plugin_event_multiplexer.py:244] Deleting accumulator '<built-in function time>'\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
